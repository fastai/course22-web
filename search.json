[
  {
    "objectID": "Resources/forums.html",
    "href": "Resources/forums.html",
    "title": "Forums",
    "section": "",
    "text": "If you need help, there’s a wonderful online community ready to help you at forums.fast.ai. Before asking a question on the forums, search carefully to see if your question has been answered before. (The forum system won’t let you post until you’ve spent a few minutes on the site reading existing topics.)\nEvery lesson has a dedicated forum thread—so that’s the place to look first to see if your question has already been answered:\nIf you’re hitting an error with your code, click the magnifying glass on the top-right of any forums page to access Search, and try searching for a few words from the error message.",
    "crumbs": [
      "Resources",
      "Forums"
    ]
  },
  {
    "objectID": "Resources/forums.html#forum-etiquette",
    "href": "Resources/forums.html#forum-etiquette",
    "title": "Forums",
    "section": "Forum etiquette",
    "text": "Forum etiquette\n\nIf you like a post, it’s better to “like” it with the :heart: rather than commenting. It saves traffic in the Forums and makes it easier for everyone to find posts.\n\nPlease be mindful of looking to see if a topic exists before starting a new thread.\nDo a quick search of the forums to see if your question is already under discussion.\n\nDo not @ people if you are not referencing them for a specific reason that requires the attention of that forum member. Be especially mindful in mentioning Jeremy.\nYou can use this thread to chat about pretty anything (except politics/religion/stuff that might start a flame war!): General chat. For other posts, please keep them to stuff that’s at least somewhat related to the course and its content.",
    "crumbs": [
      "Resources",
      "Forums"
    ]
  },
  {
    "objectID": "Resources/book.html#colab",
    "href": "Resources/book.html#colab",
    "title": "The book",
    "section": "Colab",
    "text": "Colab\nGoogle Colab is a free (with paid subscription option) platform for running Jupyter Notebooks in the cloud. You can open any chapter of the book in Colab by clicking on one of these links:\n\nChapter 1, Intro\nChapter 2, Production\nChapter 3, Ethics\nChapter 4, MNIST Basics\nChapter 5, Pet Breeds\nChapter 6, Multi-Category\nChapter 7, Sizing and TTA\nChapter 8, Collab\nChapter 9, Tabular\nChapter 10, NLP\nChapter 11, Mid-Level API\nChapter 12, NLP Deep-Dive\nChapter 13, Convolutions\nChapter 14, Resnet\nChapter 15, Arch Details\nChapter 16, Optimizers and Callbacks\nChapter 17, Foundations\nChapter 18, GradCAM\nChapter 19, Learner\nChapter 20, Conclusion",
    "crumbs": [
      "Resources",
      "The book"
    ]
  },
  {
    "objectID": "Resources/book.html#nbviewer",
    "href": "Resources/book.html#nbviewer",
    "title": "The book",
    "section": "nbviewer",
    "text": "nbviewer\nnbviewer is a free platform for reading Jupyter Notebooks. You can open any chapter of the book in nbviewer by clicking on one of these links:\n\nChapter 1, Intro\nChapter 2, Production\nChapter 3, Ethics\nChapter 4, MNIST Basics\nChapter 5, Pet Breeds\nChapter 6, Multi-Category\nChapter 7, Sizing and TTA\nChapter 8, Collab\nChapter 9, Tabular\nChapter 10, NLP\nChapter 11, Mid-Level API\nChapter 12, NLP Deep-Dive\nChapter 13, Convolutions\nChapter 14, Resnet\nChapter 15, Arch Details\nChapter 16, Optimizers and Callbacks\nChapter 17, Foundations\nChapter 18, GradCAM\nChapter 19, Learner\nChapter 20, Conclusion",
    "crumbs": [
      "Resources",
      "The book"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Practical Deep Learning",
    "section": "",
    "text": "New!\n\n\n\nWe just launched a new &gt;30 hour video course for more experienced students:\nPractical Deep Learning for Coders part 2: Deep Learning Foundations to Stable Diffusion",
    "crumbs": [
      "Practical Deep Learning"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Practical Deep Learning",
    "section": "Welcome!",
    "text": "Welcome!\nPractical Deep Learning for Coders 2022 part 1, recorded at the University of Queensland, covers topics such as how to:\n\n\n\n\n\n\n\n\n\n\nBuild and train deep learning models for computer vision, natural language processing, tabular analysis, and collaborative filtering problems\nCreate random forests and regression models\nDeploy models\nUse PyTorch, the world’s fastest growing deep learning software, plus popular libraries like fastai and Hugging Face\n\n\n\n\nThere are 9 lessons, and each lesson is around 90 minutes long. The course is based on our 5-star rated book, which is freely available online.\nYou don’t need any special hardware or software — we’ll show you how to use free resources for both building and deploying models. You don’t need any university math either — we’ll teach you the calculus and linear algebra you need during the course.\n\n\n\n\n\n\nGet started\n\n\n\nStart watching lesson 1 now!",
    "crumbs": [
      "Practical Deep Learning"
    ]
  },
  {
    "objectID": "index.html#real-results",
    "href": "index.html#real-results",
    "title": "Practical Deep Learning",
    "section": "Real results",
    "text": "Real results\nOur videos have been viewed over 6,000,000 times already! Take a look at the dozens of testimonials about our book and course by alumni, top academics, and industry experts.\n\n\n\n\n\n\n\n‘Deep Learning is for everyone’ we see in Chapter 1, Section 1 of this book, and while other books may make similar claims, this book delivers on the claim. The authors have extensive knowledge of the field but are able to describe it in a way that is perfectly suited for a reader with experience in programming but not in machine learning. The book shows examples first, and only covers theory in the context of concrete examples. For most people, this is the best way to learn. The book does an impressive job of covering the key applications of deep learning in computer vision, natural language processing, and tabular data processing, but also covers key topics like data ethics that some other books miss. Altogether, this is one of the best sources for a programmer to become proficient in deep learning.\n\n\n\nPeter NorvigDirector of Research, Google\n\n\n\nBy the end of the second lesson, you will have built and deployed your own deep learning model on data you collect. Many students post their course projects to our forum; you can view them here. For instance, if there’s an unknown dinosaur in your backyard, maybe you need this dinosaur classifier!\n\nAlumni of our course have gone on to jobs at organizations like Google Brain, OpenAI, Adobe, Amazon, and Tesla, published research at top conferences such as NeurIPS, and created startups using skills they learned here. Petro Cuenca, lead developer of the widely-acclaimed Camera+ app, after completing the course went on to add deep learning features to his product, which was then featured by Apple for its “machine learning magic”.\n\n\n\n\n\n\nGet started\n\n\n\nStart watching lesson 1 now!",
    "crumbs": [
      "Practical Deep Learning"
    ]
  },
  {
    "objectID": "index.html#your-teacher",
    "href": "index.html#your-teacher",
    "title": "Practical Deep Learning",
    "section": "Your teacher",
    "text": "Your teacher\n\n\n\n\n\n\nI am Jeremy Howard, your guide on this journey. I lead the development of fastai, the software that you’ll be using throughout this course. I have been using and teaching machine learning for around 30 years. I was the top-ranked competitor globally in machine learning competitions on Kaggle (the world’s largest machine learning community) two years running. Following this success, I became the President and Chief Scientist of Kaggle. Since first using neural networks 25 years ago, I have led many companies and projects that have machine learning at their core, including founding the first company to focus on deep learning and medicine, Enlitic (chosen by MIT Tech Review as one of the “world’s smartest companies”).\n\n\n\n\nJeremy Howard\n\n\n\n\nI am the co-founder, along with Dr. Rachel Thomas, of fast.ai, the organization behind this course. At fast.ai we care a lot about teaching. In this course, I start by showing how to use a complete, working, very usable, state-of-the-art deep learning network to solve real-world problems, using simple, expressive tools. And then we gradually dig deeper and deeper into understanding how those tools are made, and how the tools that make those tools are made, and so on… We always teach through examples. We ensure that there is a context and a purpose that you can understand intuitively, rather than starting with algebraic symbol manipulation.\n\n\n\n\n\n\nGet started\n\n\n\nStart watching lesson 1 now!",
    "crumbs": [
      "Practical Deep Learning"
    ]
  },
  {
    "objectID": "index.html#is-this-course-for-me",
    "href": "index.html#is-this-course-for-me",
    "title": "Practical Deep Learning",
    "section": "Is this course for me?",
    "text": "Is this course for me?\nPrevious fast.ai courses have been studied by hundreds of thousands of students, from all walks of life, from all parts of the world. Many students have told us about how they’ve become multiple gold medal winners of international machine learning competitions, received offers from top companies, and having research papers published. For instance, Isaac Dimitrovsky told us that he had “been playing around with ML for a couple of years without really grokking it… [then] went through the fast.ai part 1 course late last year, and it clicked for me”. He went on to achieve first place in the prestigious international RA2-DREAM Challenge competition! He developed a multistage deep learning method for scoring radiographic hand and foot joint damage in rheumatoid arthritis, taking advantage of the fastai library.\nIt doesn’t matter if you don’t come from a technical or a mathematical background (though it’s okay if you do too!); we wrote this course to make deep learning accessible to as many people as possible. The only prerequisite is that you know how to code (a year of experience is enough), preferably in Python, and that you have at least followed a high school math course.\nDeep learning is a computer technique to extract and transform data–-with use cases ranging from human speech recognition to animal imagery classification–-by using multiple layers of neural networks. A lot of people assume that you need all kinds of hard-to-find stuff to get great results with deep learning, but as you’ll see in this course, those people are wrong. Here’s a few things you absolutely don’t need to do world-class deep learning:\n\n\n\n\n\n\n\nMyth (don’t need)\nTruth\n\n\n\n\nLots of math\nJust high school math is sufficient\n\n\nLots of data\nWe’ve seen record-breaking results with &lt;50 items of data\n\n\nLots of expensive computers\nYou can get what you need for state of the art work for free\n\n\n\n\n\n\n\n\n\nGet started\n\n\n\nStart watching lesson 1 now!",
    "crumbs": [
      "Practical Deep Learning"
    ]
  },
  {
    "objectID": "index.html#the-software-you-will-be-using",
    "href": "index.html#the-software-you-will-be-using",
    "title": "Practical Deep Learning",
    "section": "The software you will be using",
    "text": "The software you will be using\nIn this course, you’ll be using PyTorch, fastai, Hugging Face Transformers, and Gradio.\nWe’ve completed hundreds of machine learning projects using dozens of different packages, and many different programming languages. At fast.ai, we have written courses using most of the main deep learning and machine learning packages used today. We spent over a thousand hours testing PyTorch before deciding that we would use it for future courses, software development, and research. PyTorch is now the world’s fastest-growing deep learning library and is already used for most research papers at top conferences.\nPyTorch works best as a low-level foundation library, providing the basic operations for higher-level functionality. The fastai library one of the most popular libraries for adding this higher-level functionality on top of PyTorch. In this course, as we go deeper and deeper into the foundations of deep learning, we will also go deeper and deeper into the layers of fastai.\nTransformers is a popular library focused on natural language processing (NLP) using transformers models. In the course you’ll see how to create a cutting-edge transfomers model using this library to detect similar concepts in patent applications.\n\n\n\n\n\n\nGet started\n\n\n\nStart watching lesson 1 now!",
    "crumbs": [
      "Practical Deep Learning"
    ]
  },
  {
    "objectID": "index.html#why-deep-learning",
    "href": "index.html#why-deep-learning",
    "title": "Practical Deep Learning",
    "section": "Why deep learning?",
    "text": "Why deep learning?\nDeep learning has power, flexibility, and simplicity. That’s why we believe it should be applied across many disciplines. These include the social and physical sciences, the arts, medicine, finance, scientific research, and many more. Here’s a list of some of the thousands of tasks in different areas at which deep learning, or methods heavily using deep learning, is now the best in the world:\n\nNatural language processing (NLP) Answering questions; speech recognition; summarizing documents; classifying documents; finding names, dates, etc. in documents; searching for articles mentioning a concept\nComputer vision Satellite and drone imagery interpretation (e.g., for disaster resilience); face recognition; image captioning; reading traffic signs; locating pedestrians and vehicles in autonomous vehicles\nMedicine Finding anomalies in radiology images, including CT, MRI, and X-ray images; counting features in pathology slides; measuring features in ultrasounds; diagnosing diabetic retinopathy\nBiology Folding proteins; classifying proteins; many genomics tasks, such as tumor-normal sequencing and classifying clinically actionable genetic mutations; cell classification; analyzing protein/protein interactions\nImage generation Colorizing images; increasing image resolution; removing noise from images; converting images to art in the style of famous artists\nRecommendation systems Web search; product recommendations; home page layout\nPlaying games Chess, Go, most Atari video games, and many real-time strategy games\nRobotics Handling objects that are challenging to locate (e.g., transparent, shiny, lacking texture) or hard to pick up\nOther applications Financial and logistical forecasting, text to speech, and much more…\n\n\n\n\n\n\n\nGet started\n\n\n\nStart watching lesson 1 now!",
    "crumbs": [
      "Practical Deep Learning"
    ]
  },
  {
    "objectID": "index.html#what-you-will-learn",
    "href": "index.html#what-you-will-learn",
    "title": "Practical Deep Learning",
    "section": "What you will learn",
    "text": "What you will learn\nAfter finishing this course you will know:\n\nHow to train models that achieve state-of-the-art results in:\n\nComputer vision, including image classification (e.g., classifying pet photos by breed)\nNatural language processing (NLP), including document classification (e.g., movie review sentiment analysis) and phrase similarity\nTabular data with categorical data, continuous data, and mixed data\nCollaborative filtering (e.g., movie recommendation)\n\nHow to turn your models into web applications, and deploy them\nWhy and how deep learning models work, and how to use that knowledge to improve the accuracy, speed, and reliability of your models\nThe latest deep learning techniques that really matter in practice\nHow to implement stochastic gradient descent and a complete training loop from scratch\n\nHere are some of the techniques covered (don’t worry if none of these words mean anything to you yet–you’ll learn them all soon):\n\nRandom forests and gradient boosting\nAffine functions and nonlinearities\nParameters and activations\nTransfer learning\nStochastic gradient descent (SGD)\nData augmentation\nWeight decay\nImage classification\nEntity and word embeddings\nAnd much more\n\n\n\n\n\n\n\nGet started\n\n\n\nStart watching lesson 1 now!",
    "crumbs": [
      "Practical Deep Learning"
    ]
  },
  {
    "objectID": "index.html#how-do-i-get-started",
    "href": "index.html#how-do-i-get-started",
    "title": "Practical Deep Learning",
    "section": "How do I get started?",
    "text": "How do I get started?\nTo watch the videos, click on the Lessons section in the navigation sidebar. The videos are all captioned; while watching the video click the “CC” button to turn them on and off. To get a sense of what’s covered in a lesson, you might want to skim through some lesson notes taken by one of our students (thanks Daniel!). Here’s his lesson 7 notes and lesson 8 notes. You can also access all the videos through this YouTube playlist.\nEach video is designed to go with various chapters from the book. The entirety of every chapter of the book is available as an interactive Jupyter Notebook. Jupyter Notebook is the most popular tool for doing data science in Python, for good reason. It is powerful, flexible, and easy to use. We think you will love it! Since the most important thing for learning deep learning is writing code and experimenting, it’s important that you have a great platform for experimenting with code.\nWe’ll mainly use Kaggle Notebooks and Paperspace Gradient because we’ve found they work really well for this course, and have good free options. We also will do some parts of the course on your own laptop. (If you don’t have a Paperspace account yet, sign up with this link to get $10 credit – and we get a credit too.)\nWe strongly suggest not using your own computer for training models in this course, unless you’re very experienced with Linux system adminstration and handling GPU drivers, CUDA, and so forth.\nIf you need help, there’s a wonderful online community ready to help you at forums.fast.ai. Before asking a question on the forums, search carefully to see if your question has been answered before.\n\n\n\n\n\n\nGet started\n\n\n\nStart watching lesson 1 now!",
    "crumbs": [
      "Practical Deep Learning"
    ]
  },
  {
    "objectID": "Lessons/lesson8a.html",
    "href": "Lessons/lesson8a.html",
    "title": "Bonus: Data ethics",
    "section": "",
    "text": "The field of data ethics has been around for a long time, and there are many academics focused on this field. It is being used to help define policy in many jurisdictions; it is being used in companies big and small to consider how best to ensure good societal outcomes from product development; and it is being used by researchers who want to make sure that the work they are doing is used for good, and not for bad.",
    "crumbs": [
      "Part 1",
      "Bonus: Data ethics"
    ]
  },
  {
    "objectID": "Lessons/lesson8a.html#video",
    "href": "Lessons/lesson8a.html#video",
    "title": "Bonus: Data ethics",
    "section": "Video",
    "text": "Video\n\n\nThis lesson, taught by Dr Rachel Thomas, the founding director of the Center for Applied Data Ethics at the University of San Francisco, was recorded in 2020 during the previous iteration of this course. It discusses some useful ways of thinking about data ethics, particularly through the lens of a number of case studies. It is based partly on chapter 3 of the book.",
    "crumbs": [
      "Part 1",
      "Bonus: Data ethics"
    ]
  },
  {
    "objectID": "Lessons/lesson8a.html#useful-links",
    "href": "Lessons/lesson8a.html#useful-links",
    "title": "Bonus: Data ethics",
    "section": "Useful links",
    "text": "Useful links\n\nDatasheets for datasets\nWeapons of math destruction\nAI Generated Faces\nPapers/repos/tools on how to check for bias\nMarkkula Center - Ethical Toolkit\nThe Pollyannish Assumption\nUnderstanding the context and consequences of pre-trial detention\nFast.ai community ethics resources & discussion\nMontreal AI Ethics Institute Weekly Newsletter",
    "crumbs": [
      "Part 1",
      "Bonus: Data ethics"
    ]
  },
  {
    "objectID": "Lessons/lesson20.html",
    "href": "Lessons/lesson20.html",
    "title": "20: Mixed Precision",
    "section": "",
    "text": "In this lesson, we dive into mixed precision training and experiment with various techniques. We introduce the MixedPrecision callback for PyTorch and explore the Accelerate library from HuggingFace for speeding up training loops. We also learn a sneaky trick for faster data loading and augmenting.\nJohno then discusses style transfer using neural networks, extracting features from different layers of a pre-trained network, and introducing content loss and the Gram Matrix. He demonstrates how to combine content loss and style loss to perform style transfer, allowing for a wide range of experimentation and artistic effects.\nLastly, Johno explores Neural Cellular Automata, inspired by Conway’s Game of Life and self-organizing systems found in nature. He implements cellular automata using hard-coded filters and neural networks with dense linear layers and convolutional layers. He trains the model using a style loss and an overflow loss, and experiments with different model sizes and loss functions for more complex and creative results.",
    "crumbs": [
      "Part 2",
      "20: Mixed Precision"
    ]
  },
  {
    "objectID": "Lessons/lesson20.html#concepts-discussed",
    "href": "Lessons/lesson20.html#concepts-discussed",
    "title": "20: Mixed Precision",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nMixed precision training\nAccelerate library from HuggingFace\nCollation function\nFaster data loading\nPre-trained neural networks\nStyle transfer\nContent loss\nGram Matrix\nNeural Cellular Automata\nCircular padding\nGradient normalization",
    "crumbs": [
      "Part 2",
      "20: Mixed Precision"
    ]
  },
  {
    "objectID": "Lessons/lesson20.html#video",
    "href": "Lessons/lesson20.html#video",
    "title": "20: Mixed Precision",
    "section": "Video",
    "text": "Video\n\n\n\nDiscuss this lesson",
    "crumbs": [
      "Part 2",
      "20: Mixed Precision"
    ]
  },
  {
    "objectID": "Lessons/lesson5.html#video",
    "href": "Lessons/lesson5.html#video",
    "title": "5: From-scratch model",
    "section": "Video",
    "text": "Video\n\n\nThis lesson is based partly on chapter 4 and chapter 9 of the book.",
    "crumbs": [
      "Part 1",
      "5: From-scratch model"
    ]
  },
  {
    "objectID": "Lessons/lesson5.html#lesson-notebooks",
    "href": "Lessons/lesson5.html#lesson-notebooks",
    "title": "5: From-scratch model",
    "section": "Lesson notebooks",
    "text": "Lesson notebooks\n\nLinear model and neural net from scratch\nWhy you should use a framework\nHow random forests really work",
    "crumbs": [
      "Part 1",
      "5: From-scratch model"
    ]
  },
  {
    "objectID": "Lessons/lesson5.html#links-from-the-lesson",
    "href": "Lessons/lesson5.html#links-from-the-lesson",
    "title": "5: From-scratch model",
    "section": "Links from the lesson",
    "text": "Links from the lesson\n\nOneR paper\nSome great Titanic notebooks: 1; 2; 3; 4",
    "crumbs": [
      "Part 1",
      "5: From-scratch model"
    ]
  },
  {
    "objectID": "Lessons/lesson18.html",
    "href": "Lessons/lesson18.html",
    "title": "18: Accelerated SGD & ResNets",
    "section": "",
    "text": "In this lesson, we dive into various stochastic gradient descent (SGD) accelerated approaches, such as momentum, RMSProp, and Adam. We start by experimenting with these techniques in Microsoft Excel, creating a simple linear regression problem and applying the different approaches to solve it. We also introduce learning rate annealing and show how to implement it in Excel. Next, we explore learning rate schedulers in PyTorch, focusing on Cosine Annealing and how to work with PyTorch optimizers. We create a learner with a single batch callback and fit the model to obtain an optimizer. We then explore the attributes of the optimizer and explain the concept of parameter groups.\nWe continue by implementing the OneCycleLR scheduler from PyTorch, which adjusts the learning rate and momentum during training. We also discuss how to improve the architecture of a neural network by making it deeper and wider, introducing ResNets and the concept of residual connections. Finally, we explore various ResNet architectures from the PyTorch Image Models (timm) library and experiment with data augmentation techniques, such as random erasing and test time augmentation.",
    "crumbs": [
      "Part 2",
      "18: Accelerated SGD & ResNets"
    ]
  },
  {
    "objectID": "Lessons/lesson18.html#concepts-discussed",
    "href": "Lessons/lesson18.html#concepts-discussed",
    "title": "18: Accelerated SGD & ResNets",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nStochastic gradient descent (SGD) accelerated approaches\n\nMomentum\nRMSProp\nAdam\n\nLearning rate annealing\nPyTorch learning rate schedulers\n\nCosine Annealing\nOneCycleLR\n\nWorking with PyTorch optimizers\nNeural network architecture improvements\n\nDeeper and wider networks\nResNets\nResidual connections\n\nData augmentation techniques\n\nRandom erasing\nTest time augmentation\n\nCreating custom schedulers and experimenting with model performance",
    "crumbs": [
      "Part 2",
      "18: Accelerated SGD & ResNets"
    ]
  },
  {
    "objectID": "Lessons/lesson18.html#video",
    "href": "Lessons/lesson18.html#video",
    "title": "18: Accelerated SGD & ResNets",
    "section": "Video",
    "text": "Video",
    "crumbs": [
      "Part 2",
      "18: Accelerated SGD & ResNets"
    ]
  },
  {
    "objectID": "Lessons/lesson18.html#lesson-resources",
    "href": "Lessons/lesson18.html#lesson-resources",
    "title": "18: Accelerated SGD & ResNets",
    "section": "Lesson resources",
    "text": "Lesson resources\n\nDiscuss this lesson\nThe course’s fashion mnist challenge topic\nExcel optimisers spreadsheet\nPapers\n\nCyclical Learning Rates for Training Neural Networks\nFixup Initialization: Residual Learning Without Normalization\nDeep Residual Learning for Image Recognition\n\nFashion-MNIST Benchmark Papers with Code",
    "crumbs": [
      "Part 2",
      "18: Accelerated SGD & ResNets"
    ]
  },
  {
    "objectID": "Lessons/lesson10.html",
    "href": "Lessons/lesson10.html",
    "title": "10: Diving Deeper",
    "section": "",
    "text": "This lesson creates a complete Diffusers pipeline from the underlying components: the VAE, unet, scheduler, and tokeniser. By putting them together manually, this gives you the flexibility to fully customise every aspect of the inference process.\nWe also discuss three important new papers that have been released in the last week, which improve inference performance by over 10x, and allow any photo to be “edited” by just describing what the new picture should show.\nIn the second half of the lesson Jeremy begins the “from scratch” implementation of Stable Diffusion. He introduces the “miniai” library which will be created by students during the course, and discusses organising and simplifying code. The lesson discusses the Python data model, tensors, and random number generation. Jeremy introduces the Wickman-Hill random number generation algorithm and compares the performance of custom and Pytorch’s built-in random number generators. The lesson concludes with creating a linear classifier using a tensor.",
    "crumbs": [
      "Part 2",
      "10: Diving Deeper"
    ]
  },
  {
    "objectID": "Lessons/lesson10.html#concepts-discussed",
    "href": "Lessons/lesson10.html#concepts-discussed",
    "title": "10: Diving Deeper",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nPapers:\n\nProgressive Distillation for Fast Sampling of Diffusion Models\nOn Distillation of Guided Diffusion Models\nImagic\n\nTokenizing input text\nCLIP encoder for embeddings\nScheduler for noise determination\nOrganizing and simplifying code\nNegative prompts and callbacks\nIterators and generators in Python\nCustom class for matrices\nDunder methods\nPython data model\nTensors\nPseudo-random number generation\n\nWickman-Hill algorithm\nRandom state in deep learning\n\nLinear classifier using a tensor",
    "crumbs": [
      "Part 2",
      "10: Diving Deeper"
    ]
  },
  {
    "objectID": "Lessons/lesson10.html#video",
    "href": "Lessons/lesson10.html#video",
    "title": "10: Diving Deeper",
    "section": "Video",
    "text": "Video",
    "crumbs": [
      "Part 2",
      "10: Diving Deeper"
    ]
  },
  {
    "objectID": "Lessons/lesson10.html#lesson-resources",
    "href": "Lessons/lesson10.html#lesson-resources",
    "title": "10: Diving Deeper",
    "section": "Lesson resources",
    "text": "Lesson resources\n\nDiscuss this lesson\nPaper walkthrough video by @johnowhitaker covering Progressive Distillation for Fast Sampling of Diffusion Models\ndiffusion-nbs repo (we continue walking through stable_diffusion.ipynb that we touched upon last time)\nFashion-MNIST reimplementation of the lesson, with notes, by @strickvl",
    "crumbs": [
      "Part 2",
      "10: Diving Deeper"
    ]
  },
  {
    "objectID": "Lessons/lesson10.html#links-from-the-lesson",
    "href": "Lessons/lesson10.html#links-from-the-lesson",
    "title": "10: Diving Deeper",
    "section": "Links from the lesson",
    "text": "Links from the lesson\n\nCourse 2022p2 repo\nProgressive Distillation for Fast Sampling of Diffusion Models\nImagic paper. Within a few hours stable diffusion versions are appearing.\nAPL: Array programming - fast.ai Course Forums",
    "crumbs": [
      "Part 2",
      "10: Diving Deeper"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson5.html",
    "href": "Lessons/Summaries/lesson5.html",
    "title": "Lesson 5",
    "section": "",
    "text": "Tabular model from scratch\nReview Titanic dataset and the two models in excel\nFrom excel to python\nClean version of notebook\n\nWhat does a clean version of the notebook look like?\n\nGet comfortable in Paperspace Gradient\n\nHow to work with jupyterlab mode instead of default mode?\nHow to swift between jupyterlab mode and jupyter notebook mode?\nLearn some useful keyboard shortcuts\n\nThings to do with clean notebook\n\nWhat are the steps or things we should do with the clean version of a course notebook?\nWhere is the non-clean version?\n\nSame notebook runs on Kaggle and everywhere\n\nHow to check whether the notebook is running on Kaggle or elsewhere?\nHow to get the data and its path right accordingly?\n\nLibraries and format setup\n\nHow much should we know about these libraries before starting?\nHow to make the printed result look nicer in cells?\n\nRead train.csv as Dataframe\n\nHow to read and display a csv file in pandas dataframe format?\n\nFind and count missing data with pandas\n\nHow to check missing data in each cell/row?\nHow to sum up missing data in each column?\n\nChoose mode value for the missing data\n\nWhat is the most common choice for replacing the missing data regardless categorical or continuous? mode\nHow to select the first mode value if there are two modes available for a column?\n\nBe proactively curious\n\nWhy it is impractical for Jeremy to explain every common function of every library used?\nWhat should you do about it?\n\nReplace missing data with mode values\n\nHow to fill in the missing data with mode values of those columns with or without creating a new dataframe?\n\nKeep things simple where we can\n\nWhy use the world’s simplest way of filling missing data?\nDoes this simplest way work most of the time?\nDo we always know the complicated way would help?\n\nDon’t throw out rows nor columns\n\nDo those filled columns sometimes turn out to matter much for the model?\nHow does fastai library help to find out about it?\n\nDescribe your data or columns\n\nHow to get a quick overview/description of your data?\nWhat do we look for in the descriptions?\n\nSee your columns in histogram\n\nWhat to do with interesting columns?\nWhat can you find out with histogram?\nWhat is long-tailed distribution of the column? What does it look like?\n\nLog transformation on long-tailed columns\n\nWhich models don’t like long-tailed distributions in the data? #data-describing\nWhat is the easiest way to turn long-tailed to centered distribution?\nWhere to find more about the log and log curve?\nWhat does log do in one sentence? 17:11\nHow to avoid the problem of log(0)? adding 1\nWhat does the column data (histogram) look like after transformed by log?\n\nMost likely long-tailed data\n\nWhat kind of data are most likely to be long-tailed which need log transformation?\n\nDescribe non-numerical columns\n\nHow to describe seemingly numerical but actual categorical columns?\nHow to describe all non-numeric columns altogether?\nWhat does this description look like? (how it differ from those of numeric data)\n\nApply coefficients on categorical columns\n\nHow to apply coefficients to categorical columns?\nWhat does it mean by applying dummy variables to categorical columns?\nWhat are the two ways of getting dummy variables and what’s Jeremy view on them?\nWhat does the dummy variable transformation of categorical variables look like ?\n\nThe secret power of name column\n\nCan a model built only on name column score No.1 in Titanic competition?\nWhere to find more about it?\nThis technique is not covered in this lecture\n\nTensor\n\nWhy focus on pytorch rather than numpy?\nWhat data format does pytorch require? How to do this data format conversion?\nWhat is a tensor? Where did it come from?\nHow to turn all independent columns into a single large tensor?\nWhat is the number type does tensor need? float\nHow to check the shape of a tensor? (num of rows and columns)\nHow to check the rank/dimensions/axis of a tensor? What is rank?\nWhat is the rank of a vector, a table/matrix, or a zero?\n\nCreate random coefficients\n\nWhy we don’t need a constant here as in excel?\nHow many coefficients we need? How we figure it out?\nHow to create a vector of randomized numbers for the coefficients?\nHow to make the coefficients value centered? Why this is important? (answered later)\n\nReproducibility of coefficients\n\nHow to create the same set of random numbers for your coefficients each time running the cell?\nWhen to and not to use random seed to make your result reproducible?\nHow not using random seed can help understand your model intuitively?\n\nBroadcasting: data * coefficients operation on GPU\n\nWhat is broadcasting? Isn’t it just matrix and vector multiplication?\nWhere did it come from?\nWhat are the benefits of using broadcasting?\nsimple code vs lots of boilerplate\ncoded and optimized in C language for GPU computation\nWhat’s the rule of broadcasting and where to find more about it?\na great blog post help understand broadcasting\n\nNormalization: make the same range of values for each column\n\nWhat would happen when the values of a column is much larger than the values of other columns?\nWhy to make every data column to have the same range of values?\nHow to achieve the same range for all column values?\nWhat are the two major ways of doing normalization?\nDoes Jeremy favor one over the other?\n\nSum up to get predictions\n\nHow to sum up the multiplication of each row with the coefficients, and do it for all rows?\nIs the summed-up number the prediction for each person/row of data?\n\nA default choice for loss function\n\nHow to make the model better? Gradient descent\nWhat is needed to do gradient descent? loss function\nWhat does a loss function do? measure the performance of coefficients\nWhat is Jeremy’s default/favorite choice for loss function?\nWhy does Jeremy always write the loss function out manually when experimenting?\n\nMake notebook readable/understandable in the future\n\nWhen to encapsulate all exploratory steps into a few functions?\nWhy keep all these exploratory steps available (Don’t delete them)?\n\nUpdate coefficients with gradient descent in Pytorch\n\nHow to ask PyTorch to do gradients on coefficients?\nHow to ask Pytorch to update values on the same coefficients tensor (not create new one)?\nWhat does loss function do besides giving us a loss value? What does it store?\nWhat function to run with loss to calculate gradients for coefficients?\nHow to access the gradients of coefficients? and how to interpret the gradients?\nHow to decide whether it is to subtract or add gradients to coefficient?\nHow to choose on the learning rate?\nHow to calc updated loss with renewed coefficients?\n\nSplit the dataset\n\nWhy did Jeremy randomly split training and validation set for Titanic dataset?\nWhy to use fastai’s random splitter function?\nHow to create the training and validation set with the splitter function?\n\nEncapsulate functions for model training\n\nHow does Jeremy create functions like init_coeffs, update_coeffs, one_epoch, train_model from the exploratory steps above?\nHow to use the train_model function to see how well the model works?\n\nTitanic dataset is a good playground\n\nWhy so?\n\nDisplay coefficients\n\nHow to display the final coefficients?\nHow to interpret it?\nCan we make some sense of the values inside?\n\nAccuracy as metrics\n\nWhy not use accuracy as loss function?\nWhat can we use accuracy function for?\nWhat threshold did Jeremy use for survival?\nHow to calculate accuracy and put it into a function?\n\nSigmoid function: ease coefficients optimization\n\nWhat you see from the predictions make you think of using sigmoid function?\nWhy sigmoid function can really make optimization easier for the model?\nWhy the two-ends plateau of the function is good for optimization? (to tolerate very large and small values of predictions rather than forcing every prediction to get closer to 1 or 0)\nWhy the straight-line middle part of the function plot is also what we want? 48:58\nHow to plot any function with just one line of code? What library is this? sympy\nHow to update calc_preds function with sigmoid function easily in Jupyter? 50:52\nWhy to make predictions to center on 0 before sigmoid function? (a reply by Jeremy)\nDo you remember what did Jeremy do to make prediction center on 0? (see how initial coefficients is defined, a cell link on Kaggle)\nWhy allow predictions to be large or small can make weights optimization easier? (Jeremy’s reply)\nHow python with Jupyter make exploratory work so easy?\nHow come the learning rate jump from 0.1 before sigmoid to 2 after using sigmoid? 51:57\nWhen or How often (as a rule) should you use sigmoid function to your prediction? 52:23\nDoes HF library specify whether they use sigmoid or not? (probably the others neither)\nWhat You need to watch out for optimization is the input and output not the middle for now. Why is that? 53:13\n\nWhat if test dataset has extra columns?\n\nWhat would the normal consequences be?\nHow does fastai deal with it for good?\n\nSubmit to Kaggle\n\nHow and why Jeremy replaced a missing value of Fare with 0?\nHow to apply the above data cleaning steps to the test set?\nHow to prepare the output column expected by Kaggle?\nHow to create the submit file expected by Kaggle?",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 5"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson5.html#linear-model-and-neuralnet-from-scratch",
    "href": "Lessons/Summaries/lesson5.html#linear-model-and-neuralnet-from-scratch",
    "title": "Lesson 5",
    "section": "",
    "text": "Tabular model from scratch\nReview Titanic dataset and the two models in excel\nFrom excel to python\nClean version of notebook\n\nWhat does a clean version of the notebook look like?\n\nGet comfortable in Paperspace Gradient\n\nHow to work with jupyterlab mode instead of default mode?\nHow to swift between jupyterlab mode and jupyter notebook mode?\nLearn some useful keyboard shortcuts\n\nThings to do with clean notebook\n\nWhat are the steps or things we should do with the clean version of a course notebook?\nWhere is the non-clean version?\n\nSame notebook runs on Kaggle and everywhere\n\nHow to check whether the notebook is running on Kaggle or elsewhere?\nHow to get the data and its path right accordingly?\n\nLibraries and format setup\n\nHow much should we know about these libraries before starting?\nHow to make the printed result look nicer in cells?\n\nRead train.csv as Dataframe\n\nHow to read and display a csv file in pandas dataframe format?\n\nFind and count missing data with pandas\n\nHow to check missing data in each cell/row?\nHow to sum up missing data in each column?\n\nChoose mode value for the missing data\n\nWhat is the most common choice for replacing the missing data regardless categorical or continuous? mode\nHow to select the first mode value if there are two modes available for a column?\n\nBe proactively curious\n\nWhy it is impractical for Jeremy to explain every common function of every library used?\nWhat should you do about it?\n\nReplace missing data with mode values\n\nHow to fill in the missing data with mode values of those columns with or without creating a new dataframe?\n\nKeep things simple where we can\n\nWhy use the world’s simplest way of filling missing data?\nDoes this simplest way work most of the time?\nDo we always know the complicated way would help?\n\nDon’t throw out rows nor columns\n\nDo those filled columns sometimes turn out to matter much for the model?\nHow does fastai library help to find out about it?\n\nDescribe your data or columns\n\nHow to get a quick overview/description of your data?\nWhat do we look for in the descriptions?\n\nSee your columns in histogram\n\nWhat to do with interesting columns?\nWhat can you find out with histogram?\nWhat is long-tailed distribution of the column? What does it look like?\n\nLog transformation on long-tailed columns\n\nWhich models don’t like long-tailed distributions in the data? #data-describing\nWhat is the easiest way to turn long-tailed to centered distribution?\nWhere to find more about the log and log curve?\nWhat does log do in one sentence? 17:11\nHow to avoid the problem of log(0)? adding 1\nWhat does the column data (histogram) look like after transformed by log?\n\nMost likely long-tailed data\n\nWhat kind of data are most likely to be long-tailed which need log transformation?\n\nDescribe non-numerical columns\n\nHow to describe seemingly numerical but actual categorical columns?\nHow to describe all non-numeric columns altogether?\nWhat does this description look like? (how it differ from those of numeric data)\n\nApply coefficients on categorical columns\n\nHow to apply coefficients to categorical columns?\nWhat does it mean by applying dummy variables to categorical columns?\nWhat are the two ways of getting dummy variables and what’s Jeremy view on them?\nWhat does the dummy variable transformation of categorical variables look like ?\n\nThe secret power of name column\n\nCan a model built only on name column score No.1 in Titanic competition?\nWhere to find more about it?\nThis technique is not covered in this lecture\n\nTensor\n\nWhy focus on pytorch rather than numpy?\nWhat data format does pytorch require? How to do this data format conversion?\nWhat is a tensor? Where did it come from?\nHow to turn all independent columns into a single large tensor?\nWhat is the number type does tensor need? float\nHow to check the shape of a tensor? (num of rows and columns)\nHow to check the rank/dimensions/axis of a tensor? What is rank?\nWhat is the rank of a vector, a table/matrix, or a zero?\n\nCreate random coefficients\n\nWhy we don’t need a constant here as in excel?\nHow many coefficients we need? How we figure it out?\nHow to create a vector of randomized numbers for the coefficients?\nHow to make the coefficients value centered? Why this is important? (answered later)\n\nReproducibility of coefficients\n\nHow to create the same set of random numbers for your coefficients each time running the cell?\nWhen to and not to use random seed to make your result reproducible?\nHow not using random seed can help understand your model intuitively?\n\nBroadcasting: data * coefficients operation on GPU\n\nWhat is broadcasting? Isn’t it just matrix and vector multiplication?\nWhere did it come from?\nWhat are the benefits of using broadcasting?\nsimple code vs lots of boilerplate\ncoded and optimized in C language for GPU computation\nWhat’s the rule of broadcasting and where to find more about it?\na great blog post help understand broadcasting\n\nNormalization: make the same range of values for each column\n\nWhat would happen when the values of a column is much larger than the values of other columns?\nWhy to make every data column to have the same range of values?\nHow to achieve the same range for all column values?\nWhat are the two major ways of doing normalization?\nDoes Jeremy favor one over the other?\n\nSum up to get predictions\n\nHow to sum up the multiplication of each row with the coefficients, and do it for all rows?\nIs the summed-up number the prediction for each person/row of data?\n\nA default choice for loss function\n\nHow to make the model better? Gradient descent\nWhat is needed to do gradient descent? loss function\nWhat does a loss function do? measure the performance of coefficients\nWhat is Jeremy’s default/favorite choice for loss function?\nWhy does Jeremy always write the loss function out manually when experimenting?\n\nMake notebook readable/understandable in the future\n\nWhen to encapsulate all exploratory steps into a few functions?\nWhy keep all these exploratory steps available (Don’t delete them)?\n\nUpdate coefficients with gradient descent in Pytorch\n\nHow to ask PyTorch to do gradients on coefficients?\nHow to ask Pytorch to update values on the same coefficients tensor (not create new one)?\nWhat does loss function do besides giving us a loss value? What does it store?\nWhat function to run with loss to calculate gradients for coefficients?\nHow to access the gradients of coefficients? and how to interpret the gradients?\nHow to decide whether it is to subtract or add gradients to coefficient?\nHow to choose on the learning rate?\nHow to calc updated loss with renewed coefficients?\n\nSplit the dataset\n\nWhy did Jeremy randomly split training and validation set for Titanic dataset?\nWhy to use fastai’s random splitter function?\nHow to create the training and validation set with the splitter function?\n\nEncapsulate functions for model training\n\nHow does Jeremy create functions like init_coeffs, update_coeffs, one_epoch, train_model from the exploratory steps above?\nHow to use the train_model function to see how well the model works?\n\nTitanic dataset is a good playground\n\nWhy so?\n\nDisplay coefficients\n\nHow to display the final coefficients?\nHow to interpret it?\nCan we make some sense of the values inside?\n\nAccuracy as metrics\n\nWhy not use accuracy as loss function?\nWhat can we use accuracy function for?\nWhat threshold did Jeremy use for survival?\nHow to calculate accuracy and put it into a function?\n\nSigmoid function: ease coefficients optimization\n\nWhat you see from the predictions make you think of using sigmoid function?\nWhy sigmoid function can really make optimization easier for the model?\nWhy the two-ends plateau of the function is good for optimization? (to tolerate very large and small values of predictions rather than forcing every prediction to get closer to 1 or 0)\nWhy the straight-line middle part of the function plot is also what we want? 48:58\nHow to plot any function with just one line of code? What library is this? sympy\nHow to update calc_preds function with sigmoid function easily in Jupyter? 50:52\nWhy to make predictions to center on 0 before sigmoid function? (a reply by Jeremy)\nDo you remember what did Jeremy do to make prediction center on 0? (see how initial coefficients is defined, a cell link on Kaggle)\nWhy allow predictions to be large or small can make weights optimization easier? (Jeremy’s reply)\nHow python with Jupyter make exploratory work so easy?\nHow come the learning rate jump from 0.1 before sigmoid to 2 after using sigmoid? 51:57\nWhen or How often (as a rule) should you use sigmoid function to your prediction? 52:23\nDoes HF library specify whether they use sigmoid or not? (probably the others neither)\nWhat You need to watch out for optimization is the input and output not the middle for now. Why is that? 53:13\n\nWhat if test dataset has extra columns?\n\nWhat would the normal consequences be?\nHow does fastai deal with it for good?\n\nSubmit to Kaggle\n\nHow and why Jeremy replaced a missing value of Fare with 0?\nHow to apply the above data cleaning steps to the test set?\nHow to prepare the output column expected by Kaggle?\nHow to create the submit file expected by Kaggle?",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 5"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson5.html#key-steps-from-linear-model-to-neuralnet",
    "href": "Lessons/Summaries/lesson5.html#key-steps-from-linear-model-to-neuralnet",
    "title": "Lesson 5",
    "section": "Key steps from linear model to neuralnet",
    "text": "Key steps from linear model to neuralnet\n\nval_indep * coeffs vs val_indep @ coeffs\n\nWhat do we know about val_indep * coeffs mean? Is it element-wise? Is it matrix and vector multiplication?\nWhat do we know about val_indep @ coeffs? Is it matrix-matrix multiplication?\nIs (val_indep * coeffs).sum(axis=1) equal to val_indep @ coeffs?\nWhat should we know about them to distinguish them properly?\nIn val_indep @ coeffs, when coeffs is a matrix, do we need to specify its shape? 59:50\nHow to initiate coefficients as a one column matrix rather than a vector?\n\nTransform existing vectors into matrix\n\nHow to turn both trns_dep and vald_dep from existing vectors to matrices which responding to coeffs matrix?",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 5"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson5.html#building-a-neural-net",
    "href": "Lessons/Summaries/lesson5.html#building-a-neural-net",
    "title": "Lesson 5",
    "section": "Building a neural net",
    "text": "Building a neural net\n\nKeep linear model and neuralnet comparable in output\n\nHow to create a layer within multi-hidden layers inside (or a matrix of coefficients rather than a vector of coefficients)?\nwhy to divide the coefficients of the multi-hidden layers by the number of layers (or the matrix of coefficients by the number of columns)?\nIs it to make sure the outputs of neuralnet and previous linear model are comparable?\n\nBuild the output layer\n\nHow to build the coeffs of the output layer with correct shape which connects with the previous layer?\nHow to decide the number of output of this output layer?\n\nTRY to getting the training started\n\nWhy Jeremy make the coefficients of the output layer to minus 0.3?\nWhat does it mean by this minus 0.3 can get the training start?\n(I guess Jeremy may tried -0.5 first, experiment to find it out)\n\nAdding Constant or not\n\nWhy we don’t need a constant for layer 1 (think of the constant of the linear model)?\nWhy we must have a constant for layer 2?\nDo coefficients of layer1, layer 2 and constants all need their own gradients initiated?\n\nBuilding the model\n\nWhat is a tuple and how it is used for grouping and separating the three coefficients?\nHow to construct the prediction function by sending data through layer 1 and layer 2 finally add constants?\n\nA neuralnet done but super fiddly\n\nHow to update all three coefficients in a loop?\nDid you notice that the learning rate changed again? (1.4, last time was 2, earlier was 0.1)\nWhat did Jeremy say getting this model work was super fiddly?",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 5"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson5.html#from-neuralnet-1-hidden-layer-to-deep-learning-with-2-hidden-layers",
    "href": "Lessons/Summaries/lesson5.html#from-neuralnet-1-hidden-layer-to-deep-learning-with-2-hidden-layers",
    "title": "Lesson 5",
    "section": "From neuralnet (1 hidden layer) to deep learning with 2 hidden layers",
    "text": "From neuralnet (1 hidden layer) to deep learning with 2 hidden layers\n\nInitialize coefficients of all hidden layers and constants\n\nHow to initialize coefficients of 2 hidden layers and 1 output layer and constants, and get gradients ready for all of them, in one compact function?\nWhat are the shape of each coefficient matrix?\n\nBuilding the 2 hidden layer model\n\nWhat are activation functions?\nWhat are the activation functions for 2 hidden layers?\nWhat is the activation function for the output layer?\nWhat is the most common mistake on applying activation function to the final layer?\n\nTrain the model\n\nDon’t forget to update gradients\nWhich are those numbers Jeremy still have to fiddle to get right?\nDid this deep learning model improve on the loss and accuracy?\n\nDissect and Experiment large functions\n\nHow to experiment on a large function like the init_coeffs by breaking it into small pieces and running them?\n\nTabular datasets: where deep learning not shining\n\nHow should we think about that both neuralnet and deep learning models aren’t better?\nWhat does it mean that a carefully designed algo beat all deep learning models in Titanic competition?\nWhat datasets do deep learning generally perform better?",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 5"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson5.html#framework-dl-without-super-fiddling-notebook",
    "href": "Lessons/Summaries/lesson5.html#framework-dl-without-super-fiddling-notebook",
    "title": "Lesson 5",
    "section": "Framework: DL without super fiddling notebook",
    "text": "Framework: DL without super fiddling notebook\n\nWhy use framework rather than from scratch\n\nWhy you should use a library framework in real life rather than building yourself like the above?\nWhen to do it from scratch?\nWhat can a framework do for us?\nCan it automate the obvious things like initialization, learning rate, dummy variable, normalization, etc?\nCan I still make choice on the not-so obvious things?\n\nFeature engineering with pandas\n\nWhat is the feature engineering with pandas look like?\nHow and where does Jeremy suggest on digging in pandas?\n\nAutomate the obvious when preparing dataset\n\nHow framework make cateorifying data, filling missing data, normalization automatic?\nHow to specify the dependent column to be a category?\n\nBuild multi-hidden layers with one line of code\n\nHow to specify the shapes of two hidden layers with just two numbers?\nDo you only need to specify accuray without worrying about loss and activations?\n\nAutomate the search of best learning rate\n\nHow does fastai help you find the range where best learning rate locates?\nHow should you pick the learning rate from the range?",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 5"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson5.html#predict-and-submit-with-ease",
    "href": "Lessons/Summaries/lesson5.html#predict-and-submit-with-ease",
    "title": "Lesson 5",
    "section": "Predict and Submit with ease",
    "text": "Predict and Submit with ease\n\nAutomate transformation of test set in one line of code\n\nHow to automatically apply all transformations done to training and validation sets to test set?",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 5"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson5.html#experiment-with-ensembling",
    "href": "Lessons/Summaries/lesson5.html#experiment-with-ensembling",
    "title": "Lesson 5",
    "section": "Experiment with Ensembling",
    "text": "Experiment with Ensembling\n\nEnsemble is easy with fastai\n\nDoes framework save so much of fiddling so that experimenting with some advanced ideas become easier?\nWhat is ensembling?\nIs it to combine multiple models and combine their predictions?\n\nThe simplest ensemble\n\nWhat does a simple ensemble look like?\nHow to build, run and predict with 5 models with ease?\nHow different are those 5 models? (only initial coefficients are different)\nHow to combine their predictions?\nHow much improvement does this simple ensemble get us to?\n\nWays to combine the predictions\n\nWhy not use mode but mean?\nWhat are 3 ways of combining the predictions?\nDoes one is better than the others?\nWhat’s Jeremy’s suggestion?",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 5"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson5.html#how-random-forest-really-work",
    "href": "Lessons/Summaries/lesson5.html#how-random-forest-really-work",
    "title": "Lesson 5",
    "section": "How Random Forest really work",
    "text": "How Random Forest really work\n\nIs this a good place to also learn pandas and numpy?\nWhy Random Forest\n\nWhat is the history of Random Forest and Jeremy?\nWhat does Jeremy think of random forest?\nWhy random forest is so much easy and better?\nWhy the seemingly simple logistic regression is so easy to get wrong?\n\nPandas categorical function\n\nHow to import all the libraries you need at once?\nHow to do fillna with pandas and log with numpy?\nWhat does panda categorical function do for us?\nWhat’s friendly display after the function applied?\nWhat’s the actual data transformation under the hood?\nKey points to make: No dummy variables, Pclass no long needed to be viewed as categories\n\nBinary splits: bases of random forest\nA binary splits on gender\nBuild a binary splits model on gender with sklearn\nBuild a binary splits model on Ticket prices with sklearn\nBuild a score machine on binary splits regardless categorical or continuous\n\nWhat is a good split?\nIs it good that within each group their dependent values are similar?\nHow to measure the similarity of values within a group? std\nHow to compare standard deviations between two groups appropriately? (multiply by size)\nHow to calc the score for evaluating the splits based on the value of combined std of two groups?\n\nAutomate the score machine on all columns\n\nHow to find the best binary splits by trying out all possible split points of a column?\n\n1R model as the baseline\n\nWhat is a random forest? and what is a random forest?\nWhat is 1r model?\nhow good was it in the 90s of ML world?\nShould we always go for complicated models?\nShould we always start with a 1r model as a baseline model for our problem?",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 5"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson1.html",
    "href": "Lessons/Summaries/lesson1.html",
    "title": "Lesson 1",
    "section": "",
    "text": "Daniel 深度碎片 on forums.fast.ai has been kind enough to create summaries, in the form of a list of questions, of every lesson. You can use these summaries to remind yourself what you learned in each lesson, or to preview a lesson before you watch it. Here’s the lesson 1 summary:\n\nWelcome to Part 1 2022 course\nWere computers smart enough to determine photos of birds before 2015?\nHow to download and display a photo of a bird from DuckDuckGo using simple codes?\nWhat photos/images are actually made of, at least for computers?\nHow to create two folders named ‘bird’ and ‘forest’ respectively under a larger folder ‘dest’? How to download 200 images for each category? How to resize and save those images in respective folders?\nHow to find broken images and then remove or unlink them from their folders?\nHow to create a DataBlock which prepares all the data for building models? How to display the images in a batch?\nHow to build a model and train/finetune it on your local computer?\nHow to predict or classify a photo of bird with a model?\nHow to get started running and playing around the codes and models immediately and effortlessly?\nWhy should you read lecture questionnaires before studying the lecture?\nHow do you search and locate a particular moment inside a lecture video?\nCan you create an original masterpiece painting by simply utterring some artistic words?\nCan you believe that models today can explain your math problems not just give you a correct answer? Can you believe that models today can help you get a joke?\nJeremy and fastai community make serious effort in help beginners continuously.\nDo you want to know how to make the most out of fastai?\nDo you know people learn naturally (better) with context rather than by theoretical curriculum? Do you want this course to make you a competent deep learning practitioner by context and practical knowledge? If you want theory from ground up, go to part 2 fastai 2019\nDo you know that learning the same thing in different ways betters understanding?\nWhy you must take this course very seriously? (Personally, I think it’s truly a privilege to be taught by Jeremy and to be part of the fastai family. I didn’t appreciate it enough as I should 4 years ago.)\nWhy did we need so many scientists from different disciplines to collaborate for many years in order to design a successful model before deep learning?\nWhy can deep learning create a model to tell bird from forest photos in 2 minute which was the impossible before 2015? Would you like to see how much better/advanced/complex are the features discovered by deep learning than groups of interdisciplinary scientists?\nAre all things are data, sound, time (series), movement? Are images are just one way of expressing data? Why not store or express data (of sound, time, movement) in the form of images? Can imaged based algos learn on those images no matter how weird they appear to humans?\nCan I do DL with no math (I mean with high school math)? Can I train DL models with hand-made data (&lt;50 samples)? Can I train state of art models for free (literally)?\nWhich should I invest my life in DL software field, Pytorch or Tensorflow?\nWhy should you use fastai over pure pytorch? Don’t you want to write less code, make less error, achieve better result? Don’t you want a robust and simple tool used by your future colleagues and bosses?\nWhy is jupyter notebook the most loved and tested coding tool for DL? Do you want Jeremy to show you how to use Jupyter notebook hand by hand?\nHow to make sure your notebook is connected in the cloud? How to make sure you are using the latest updated fastai? #best-practice\nDoesn’t fastai feel like python with best practices too? How to import libraries to download images? How to create and display a thumbnail image? Always view your data at every step of building a model #best-practice How to download and resize images? Why do we resize images? #best-practice\nWhy a real world DL practitioner spend most of the valuable/productive time preparing data rather than tweaking models? Can super tiny amount of models solve super majority of practical problems in the world? Have fastai selected and prepared the best models for us already?\nDoes Jeremy add best practices of other programming languages into fastai? Jeremy loves functional programming\nHow fastai design team decide what tasks should DataBlock do? task 1: Which blocks of data do DataBlock need to prepare for training? task 2: How should DataBlock get those data, or by what function/tool? task 3: Should we always ask DataBlock to keep a section of data for validation? task 4: Which function or method should DataBlock use to get label for y? task 5: Which transformation should DataBlock apply to each data sample? task 6: Does dataloader do the above tasks efficiently by doing them in thousands of batches at the same time with the help of GPUs?\nWhat is the most efficient way of finding out how to use e.g., DataBlock properly? How to learn DataBlock thoroughly?\nWhat do you give to a learner, e.g., vision_learner?\nIs fastai the first and only framework implement TIMM? Can you use any model from TIMM in your project? Where can you learn more of TIMM?\nWhat is a pretrained model, Resnet18? What did this model learn from? What come out of this model’s learning? or what is Kaggle downloading exactly?\nWhat exactly does fine tuning do to the pretrained model? What does fine-tuning want the model learn from your dataset compared with the pretrained dataset?\nHow to use the fine tuned model to make predictions?\nCan we fine tune pretrained CV models to tell us the object each and every pixel on a photo belong to?\nWhy do we need specialized DataLoaders like SegmentationDataLoaders given DataBlock?\nWhat can tabular analysis do? Can we use a bunch of columns to predict another column of a table? How do you download all kinds of dataset for training easily with fastai? untar_data What are the parameters for TabularDataLoaders? What is the best practice show_batch of fastai learned from Julia (another popular language)? Why to use fit_one_cycle instead of fine_tune for tabular dataset?\nCan we use collaborative filtering to make movie recommendations for users? How does recommendation system work? Can collaborative filtering models learn from data of similar music users and recommend/predict music for new users based on how similar they are to existing users?\nHow to download dataset for collaborative filtering models? How to use CollabDataLoaders? How to build a collaborative filtering model with collab_learner? What is the best practice for setting y_range for collab_learner? #best-practice If in theory no reason to use pretrained collab models, and fine_tune works as good as fit or fit_one_cycle, any good explanations for it? #question How to show results of this recommendation model using show_results?\nWhat can Deep Learning do at the present? What are the tasks that deep learning may not be good at?\nHas the basic idea of deep learning changed much since 1959?\nWhat did we write into programs/models before deep learning? How to draw chart in jupyter notebook?\nWhat is a model? What are weights? How do data, weights and model work together to produce result? Why are the initial results are no good at all? Can we design a function to tell the model how good it is doing? loss function Then can we find a way to update/improve weights by knowing how bad/good the model is learning each time from the data? If we can iterate the cycle multiple times, can we build a powerful model?\nHomework: Run notebooks, especially the bird notebook. Create something interesting to you based on the bird notebook. Read the first chapter of the book. Be inspired by all the amazing student projects.",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 1"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson3.html",
    "href": "Lessons/Summaries/lesson3.html",
    "title": "Lesson 3",
    "section": "",
    "text": "Introduction and survey\n“Lesson 0” How to fast.ai\n\nWhere is Lesson 0 video?\nWhat does it to do with the book ‘meta learning’ and fastai course?\n\nHow to do a fastai lesson?\n\nWatch with note\nRun the notebook and experiment\nReproduce the notes from the codes\nRepeat with a different dataset\n\nHow to not self-study?\n\nphysical and virtual study group\nstudy with people on forum\nLearn with social interactions is better than self-study\n\nHighest voted student work\n\nMany interesting projects to check out\n\nJeremy’s Pets breeds detector\n\nJeremy’s Pets repository\nWhat you should do with this App example?\n\nPaperspace: your DL workstation in cloud!\n\nDoes Jeremy speak highly of it? and Why?\n\nJupyterLab: real beginner friendly\n\nWhy JupyterLab is so good for beginners to take advantage of?\n\nMake a better pet detector\n\nAfter training, we should think about how to improve it\n\nComparison of all (image) models\n\nDid anyone compared most of the image models and shared the finding?\nWhere to find the notebook for comparison?\nWhich 3 criteria are used for comparison?\n\nTry out new models\n\nHow to select and try out models with high scores\nWhere is the train.ipynb file?\nHow to try models on TIMM?\nHow to compare them by loss?\nWhy this model is actually impressive?\nWhat can the name of a model tell us?\nWhy Jeremy only train 3 epochs? 18:58\n\nGet the categories of a model\n\nHow to get labels or categories info from the model?\nThe rest is we learnt from last lecture.\n\nWhat’s in the model\n\nWhat two things are stored in the model?\n\nWhat does model architecture look like?\nParameters of a model\n\nHow to zoom in on a layer of a model?\nHow to check out the parameters of a layer?\nWhat does a layer’s parameters look like?\n\nThe investigating questions\n\nWhat are the weights/numbers?\nHow can they figure out something important?\nWhere is the notebook on how neuralnet work\n\nCreate a general quadratic function\n\nHow to create a general function to output any specific quadratic function by changing 3 parameters?\nHow to generate result from a specific quadratic function by changing 1 parameter?\nWhy do we create such a general (quadratic) function with multiple unknown parameters rather than directly writing a particular quadratic function with specific coefficients?\n\nFit a function by good hands and eyes\n\nWhat does fit a function mean? (search better parameters based on dataset)\nHow to create a random dataset?\nHow to fit a general quadratic function to the dataset by changing 3 parameters with jupyter widgets by hand?\nWhat is the limitation of this manual/visual approach?\nwhere is this notebook\n\nLoss: fit a function better without good eyes\n\nWhy do we need loss or loss function?\nWhat is mean squared error?\nHow does loss help the hand/visual approach to be more accurate and robust?\n\nAutomate the search of parameters for better loss\n\nHow do we know which way and by how much to update parameters in order to improve on loss?\nCan you find enough derivative material on Khanacademy?\nWhat exactly do you need to know about derivative for now according to Jeremy? 34:26\nWhat is the slope or gradient?\nDoes pytorch do derivative or calc slope/gradient for us?\nHow to create a function to output sme loss on a general quadratic function? 35:02\nWhat do you need to know about tensor related to derivatives for now according to Jeremy? 36:02\nHow to create a rank 1 tensor (a list to store numbers) to store parameters of the quadratic function? 36:49\nHow to ask pytorch to prepare the calculation of gradients for these parameters? 37:10\nHow to actually calculate gradients for each parameter based on the loss achieved by this specific function (3 specific parameters) against the whole dataset? 37:38\nIn other words, this time when we calculate loss we can easily get the gradient for each parameter as well.\nWhat does the gradient value mean for each parameter? 38:34\nHow to update parameters into new values with the gradients produced by the loss? 39:18\nHow to automate the process above to find better parameters to achieve better loss? 41:05\nWhy this automation is called gradient descent?\nnotebook\n\nThe mathematical functions\n\nBesides dataset, loss function, derivative, what is also very crucial in finding/calculating those parameters?\nWhy we can’t simply use quadratic functions for it?\n\nReLu: Rectified linear function\n\nReal world powerful models demands complex parameters and also complex functions, how complex a function can we come up?\nIs it possible to come up an infinitely complex function by simply doing addition of extremely simple functions?\nWhat could such extremely simple function look like?\nWhat is rectified linear function? How simple it is? What is linear and which part is rectified?\nWhat does rectified linear function look like in plot?\nHow to adjust the 2 parameters of the function by hand with widget?\nWhat the function could look like under different parameters? 44:46\n\nInfinitely complex function\n\nHow powerful can the addition of extremely simple functions be?\nHow to create a double rectified linear function (double relu) and adjust 4 parameters by hand with widget?\nHow much more flexible does this double relu function look compared to a single rectified linear function?\nCan you imagine how complex can a function be when millions of rectified linear functions are added?\n\n2 circles to an owl\n\na very concise summarization of sewing fundamental ideas together for deep learning\n\nA chart of all image models compared\n\nCan it be done with brute force computation with simple code?\nDoes Jeremy look for the model comparison chart for best models?\nWhat is the wrong way of using the comparison chart by students? 50:45\nHow does Jeremy use the chart?\nhow does Jeremy decides which models to try out step by step?\n\nDo I have enough data?\n\nDid you already build a model and train on your own dataset?\nIs the result good enough for you?\nWhat is the mistake the DL industry often make on this issue? 52:55\nWhat is Jeremy’s suggestion?\nHow and what could semi-supervised learning and data augmentation be helpful?\nWhat about labeled and unlabeled data?\n\nInterpret gradients in unit?\n\nHow much does the loss go down when parameter a increase by unit of 1? 55:24\n\nLearning rate\n\nWhy we don’t update parameter values in large steps?\nWhy does Jeremy draw a quadratic function to refer to the model when zooming in very close into the complex function?\nWhat would happen when update parameters with large values? 57:19\nDoes large drop on loss necessarily demand large value increase of parameter according to the quadratic nature?\nWhat is learning rate? Why we need it to be small? How to pick a good value of it? 58:07\nWhat would happen if your learning rate is too big?\nWhat would happen when too small?\n\nbreak\nMatrix multiplication\n\nWhen the model requires millions of rectified linear functions, how to calculate fast enough?\nWhat is actually needed from linear algebra to do DL 1:01:33\nHow easy it is to do matrix multiplication? 1:01:51\nWhat are the dataset and parameters in the matrix multiplication?\nDoes matrix multiplication do the rectified part for you?\nWhat GPU is good at? 1:03:49\n\nBuild a regression model in spreadsheet\n\nIntro to Titanic Competition on Kaggle 1:05:01\nWhat is the dataset 1:05:18\nWhat to do with the train.csv file?\nHow to clean the dataset a little bit?\nHow to transform the dataset for matrix multiplication? 1:07:17\nHow to prepare parameters for matrix multiplication? 1:08:50\nWhat’s wrong with the much larger value of the column ‘Fare’ compared to other columns? 1:09:35\nWhat to do with the values of ‘Fare’ and similarly the values of ‘Age’?\nWhat is normalizing the data?\nDoes fastai do all these normalizations for us? Will we learn how fastai does it in the future?\nWhy to apply log to values of ‘Fare’? 1:10:59\nWhy do we need values to be evenly distributed?\nHow to do mmult on dataset and parameters in spreadsheet? 1:11:56\nHow to use mmult instead of addition to add a constant?\nWhat does the result of our model look like? 1:13:41\nDoes Jeremy simply use a linear regression for the model, not even a relu?\nCan we solve regression with gradient descent? How do we do it?\n\nBuild a neuralnet by adding two regression models\n\nWhat does it take to turn a regression model into a neuralnet?\nWhy we don’t add up the results of two linear functions?\nWhy we only add the results together after they are rectified?\nWhat does the model prediction look like?\nNow we need to update the parameters for two linear functions, not just one.\n\nMatrix multiplication makes training faster\n\nHow to make the training to do mmult rather than addition of linear multiplications in spreadsheet?\n\nWatch out! it’s chapter 4\n\nPlease do try out Titanic competition\nWhy chapter 4 drove away most of people?\nWays to work out the spreadsheet yourself\n\nCreate dummy variables of 3 classes\n\nDo we only need 2 columns/classes for a dummy variable with 3 classes?\n\nTaste NLP\n\nWhat do Natural Language Processing models do?\nWhat project opportunities do non-En-speaker students have?\nWhat tasks can NLP do? 1:25:57\n\nfastai NLP library vs Hugging Face library\n\nHow do these two libraries differ?\nWhy we use transformer library in this lecture?\n\nHomework to prepare you for the next lesson",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 3"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson7.html",
    "href": "Lessons/Summaries/lesson7.html",
    "title": "Lesson 7",
    "section": "",
    "text": "We have explored the simplest neural net with fully connected linear layers in earlier lectures. In this lecture we will focus on tweaking first and last layers, in the next few weeks on tweaking middle part of the neuralnet.\nReview of the notebook Road to Top part 2 and congrats to fastai students beat Jeremy on 1st and 2nd\nWhat are the benefits of using larger models? What are the problems of larger models? (use up GPU memory as GPU is not as clever as CPU to find ways to free itself; so large model needs very expensive GPU) What can we do about it when GPU out of memory? first, to restart the notebook; then Jeremy is about to show us a trick to enable us to train extra large models on Kaggle, Wow!\nHow big is Kaggle GPU? Do you have to run notebooks on kaggle sometimes for example code competitions? Why it is good and fair to use Kaggle notebook to win leaderboard?\nHow did Jeremy use a 24G GPU to find out what can a 16G GPU do? How did Jeremy find out how much GPU memory will a model use? How did Jeremy choose the smallest subgroup of images as the training set? Will training the model longer take up more memory? (No) So, smallest training set + 1 epoch training can quickly tell us how much memory is needed for the model.\nJeremy then trained different models to see how much memories they used up. How much memory does convnext-small model take? Which line of code does Jeremy use to find out the GPU memory used up by the model? Which two lines of code does Jeremy use to free unnecssarily occupied memories GPU so that you don’t need to restart the kernel to run the next model?\n\n\n\nWhat if a model causes a crash problem of cuda out of memory? What is GradientAccumulation? What is integer divide? (//).\n\n\n\nWhat is the problem of using smaller batch size? (smaller batch size, larger volatility of learning rate and weights) How can we make the model train in smaller batch size as if it is in large batch size? How to explain GradientAccumulation in code?\n\n\n\n\nWhat is the implication of using GradientAccumulation? How much difference is the numeric result between using GradientAccumulation and not? What is the main cause for the difference?\nMore questions: it should be count &gt;= 64 in the code above when doing GradientAccumulation; lr_find uses batch size from the DataLoader;\nWhy not just use a smaller batch size instead of GradientAccumulation? What is the rule of thumb for picking batch sizes? How about adjusting learning rate according to the batch size?\nHow did Jeremy use GradientAccumulation to find out how many accum is needed to run those large models on Kaggle’s 16G GPUs? (accum=1 always out of memory, but accum=2 works for all large models).",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 7"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson7.html#gradient-accumulation-and-gpu-memory",
    "href": "Lessons/Summaries/lesson7.html#gradient-accumulation-and-gpu-memory",
    "title": "Lesson 7",
    "section": "",
    "text": "We have explored the simplest neural net with fully connected linear layers in earlier lectures. In this lecture we will focus on tweaking first and last layers, in the next few weeks on tweaking middle part of the neuralnet.\nReview of the notebook Road to Top part 2 and congrats to fastai students beat Jeremy on 1st and 2nd\nWhat are the benefits of using larger models? What are the problems of larger models? (use up GPU memory as GPU is not as clever as CPU to find ways to free itself; so large model needs very expensive GPU) What can we do about it when GPU out of memory? first, to restart the notebook; then Jeremy is about to show us a trick to enable us to train extra large models on Kaggle, Wow!\nHow big is Kaggle GPU? Do you have to run notebooks on kaggle sometimes for example code competitions? Why it is good and fair to use Kaggle notebook to win leaderboard?\nHow did Jeremy use a 24G GPU to find out what can a 16G GPU do? How did Jeremy find out how much GPU memory will a model use? How did Jeremy choose the smallest subgroup of images as the training set? Will training the model longer take up more memory? (No) So, smallest training set + 1 epoch training can quickly tell us how much memory is needed for the model.\nJeremy then trained different models to see how much memories they used up. How much memory does convnext-small model take? Which line of code does Jeremy use to find out the GPU memory used up by the model? Which two lines of code does Jeremy use to free unnecssarily occupied memories GPU so that you don’t need to restart the kernel to run the next model?\n\n\n\nWhat if a model causes a crash problem of cuda out of memory? What is GradientAccumulation? What is integer divide? (//).\n\n\n\nWhat is the problem of using smaller batch size? (smaller batch size, larger volatility of learning rate and weights) How can we make the model train in smaller batch size as if it is in large batch size? How to explain GradientAccumulation in code?\n\n\n\n\nWhat is the implication of using GradientAccumulation? How much difference is the numeric result between using GradientAccumulation and not? What is the main cause for the difference?\nMore questions: it should be count &gt;= 64 in the code above when doing GradientAccumulation; lr_find uses batch size from the DataLoader;\nWhy not just use a smaller batch size instead of GradientAccumulation? What is the rule of thumb for picking batch sizes? How about adjusting learning rate according to the batch size?\nHow did Jeremy use GradientAccumulation to find out how many accum is needed to run those large models on Kaggle’s 16G GPUs? (accum=1 always out of memory, but accum=2 works for all large models).",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 7"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson7.html#creating-an-ensemble",
    "href": "Lessons/Summaries/lesson7.html#creating-an-ensemble",
    "title": "Lesson 7",
    "section": "Creating an ensemble",
    "text": "Creating an ensemble\n\nHow did Jeremy put all the models and their settings together for experimenting later? Do we have to use the size of the model’s specification for now and how about in the future?\n\n\n\n\nHow to run all the models with specifications without running out of memory\n\n\n\nWhy does Jeremy don’t use seed=42 here in training? What is the effect?\nWhat is ensemble or bagging of different good deep learning architectures? Why it is useful?\nHow to do the ensemble of different deep learning models?\n\n\n\n\n\n\nWhy should we improve and submit to Kaggle everyday? How the submission history can help trace your models developments and improvement?\nMore questions: What is k-fold cross-validation and how can it be applied in this case? Why does Jeremy don’t use it?\nAre there any drawbacks of GradientAccumulation? Any GPU recommendations?\nIn part 2 Jeremy may cover how to train a smaller model to do well as in large models for faster inference",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 7"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson7.html#multi-target-model",
    "href": "Lessons/Summaries/lesson7.html#multi-target-model",
    "title": "Lesson 7",
    "section": "Multi-target model",
    "text": "Multi-target model\n\nHow to set the data split and item and batch transformations?\n\n\n\nHow to create a model to predict both disease and variety types? Can we see predicting both disease and variety in terms of predicting 20 things, 10 for disease, 10 for variety?\nWhat does the new model (and new dataloaders) need now to make predictions on disease?\n\n\n\n\nWhen and how to provide our own loss function? fastai can detect appropriate loss for your datalaoders and use it by default in simple cases. In this special case, How do we create and use our custom loss for the new model?",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 7"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson7.html#cross-entropy-and-softmax",
    "href": "Lessons/Summaries/lesson7.html#cross-entropy-and-softmax",
    "title": "Lesson 7",
    "section": "Cross-entropy and softmax",
    "text": "Cross-entropy and softmax\n\nWhat does F.cross_entropy do exactly? This function belong to the first and last layer, therefore we must understand them. What is the raw output of the model of predicting 5 things?\n\n\n\nWhat is the formula of softmax and How to calculate it in the spreadsheet?\n\n\n\nWhat is the problem of softmax? How does it make the obvious wrong prediction when given a cat image to the bear classifier?\nWhat can we do about the problem of the softmax above? (all prediction probabilities not adding up to 1). When do you use softmax and when not to?\nWhat is the first part of the cross_entropy loss formula?\n\n\n\nHow to calculate cross-entropy from softmax?\n\n\n\nHow to calculate binary-cross-entropy? How to understand its formula in predicting whether it is a cat or non-cat image? How to finally get the binary cross-entropy loss of a batch of 5 images?\n\n\n\nWhat are two versions of cross-entropy in pytorch? and when to use each version? Which version do we use here?",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 7"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson7.html#multi-target-activations",
    "href": "Lessons/Summaries/lesson7.html#multi-target-activations",
    "title": "Lesson 7",
    "section": "Multi-target activations",
    "text": "Multi-target activations\n\nWith a dataloader having two targets, our new model needs to be informed what exactly is the loss func, metrics, and the size of output?\n\n\n\nHow to create a learner for prediction two targets or 20 items? How does a learner use disease and variety losses to know which 10 items are disease predictions and which 10 are variety predictions? How to combine two loss functions together? How to understand the combined loss?\n\n\n\n\n\n\nHow to calc error rate for disease types and variety types? How to put them together and display them during training?\n\n\n\nHow to make the new learner and how did it train? Why the multi-task model didn’t improve and even a little worse than the previous model? Why training the multi-task model longer could improve the accuracy on disease prediction? Why predicting a second thing together could help improve the prediction of the first thing? Using multi-task model did improve the result in a Kaggle fish prediction competition Jeremy did before. What are the reasons or benefits for building multi-task models?\n\n\n\nHow to make multi-task modeling less confusing to you? (build a multi-task for Titanic dataset from scratch; explore and experiment this notebook) by Chris Said of binary-cross-entropy?",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 7"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson7.html#collaborative-filtering",
    "href": "Lessons/Summaries/lesson7.html#collaborative-filtering",
    "title": "Lesson 7",
    "section": "Collaborative filtering",
    "text": "Collaborative filtering\n\nCollaborative filtering deep dive as chp 8 without change. What is the dataset used? Which version of the data we are using? How to read a tsv file using pandas? How to read/understand the dataset content/columns? recommendation system industry and Radek. How does Jeremy prefer to see the data? (cross tabulated) Why the image Jeremy talking about his preferred way of seeing the data has so few empty or missing data?\n\n\n\n\n\nHow to fill in the missing data or gaps in the cross tabulated dataset? How to figure out whether a new user would like a particular movie which he/she has not watched before? Can we figure out what kind/genre of movie is the particular movie we are talking here? What does the type probabilities of a movie look like? What does a user’s preference probabilities look like? If we match the two sets of probabilities up, can we know how much does the user like the movie? How do we calculate that?\n\n\n\n\n\nSo far so good, what is the problem of the approach of doing dot product between user preference probabilities and movie type probabilities to find out our new user’s rating of the movie? (we don’t know neither of the probabilities). How are we going to deal with this problem? Can we create such movie type probabilities without knowing even the types?\nWhat is the latent factors? If I don’t know anything about the movies, can we use SGD (stochastic gradient descent) to find them? Can we create a random 5 numbers as a movie’s 5 latent factors for describing the types of the movie, and figure them out later? Can we create latent factors for each user too? Now how to calc the probability of a user likes a movie? (mmult or dot product between two groups of latent factors).\n\n\n\n\n\n\nNow the mmult or dot product can give us the prediction of how much a user likes a movie, so we can compare predictions with true label. What to do when there is a missing label or data? (we make the prediction empty or zero). Can we use SGD to improve the latent factors by comparing predictions with labels using a loss function? How to use excel solver to update latent factors using SGD and the loss?\n\n\n\n\n\nWhy excel is so slow on calc gradients with even small dataset? What is the basis of collaborative filtering? (if we know A likes (a, b, c) and B likes (a, b, c), then if A likes (d, e), maybe B likes (d, e) too). - Is the cosine of an angle between two vectors is the same thing as the dot product? - How do we do the things above in pytorch as they have different data format from excel? What does the dataset would look like in pytorch?",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 7"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson7.html#embeddings",
    "href": "Lessons/Summaries/lesson7.html#embeddings",
    "title": "Lesson 7",
    "section": "Embeddings",
    "text": "Embeddings\n\nWhat is embedding? What are embedding matrix, user embeddings, and movie embeddings? (embeddings = look up something in an array). The more intimidating words created in a field, the less intimidating the field actually is.\n\n\n\nWhat does our dataset look like before building a dataloaders on it? How to create a dataloaders for collaborative filtering using CollabDataloaders.from_df? What does its show_batch look like? How do we create the user and movie latent factors algetother?\n\n\n\n\n\n\nHow do you choose the number of latent factors in fastai?\nHow to understand looking up in excel for latent factors and doing dot product with one-hot embeddings are actually the same thing? Can we think of embeddings as a computational shortcut to multiply something by a one-hot-encoded vector? Can we think of embedding as a cook math trick of speeding up the matrix multiplication with dummy variables (without creating dummy variables nor one-hot encoded vector).",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 7"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson7.html#object-oriented-programming",
    "href": "Lessons/Summaries/lesson7.html#object-oriented-programming",
    "title": "Lesson 7",
    "section": "Object oriented programming",
    "text": "Object oriented programming\n\nHow to build a collaborative filtering model from scratch? How do we create a class? (as a model is a class). How do we initiate a class object by __init__? Does __init__ tell us what parameters to give in order to create a class instance? How does the class function say do? What is a super class? Where do we put it when creating a class? What does it give us? What is the super class (Module) for pytorch and fastai to use when creating a class? What does the DotProduct class look like?\n\n\n\n\n\n\nHow to understand the forward function in the DotProduct class? What does .sum(dim=1) mean? (sum each row).",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 7"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson7.html#improving-collaborative-filtering",
    "href": "Lessons/Summaries/lesson7.html#improving-collaborative-filtering",
    "title": "Lesson 7",
    "section": "Improving collaborative filtering",
    "text": "Improving collaborative filtering\n\nHow to create a collab learner and start training? The training is very fast even on CPU.\n\n\n\n\nWhy this collab model above is not great? (people who give ratings are people who love movies, they don’t rarely give 1, but many high ratings. Whereas the predictions have many occassions with ratings over 5). Review the sigmoid usage. How can we do sigmoid transformation to the predictions? How does this sigmoid work? Why do we use the up limit of the range 5.5 instead of 5? Does adding sigmoid always improve the result?\n\n\n\nWhat interesting things did Jeremy observe from the dataset? (some users like to give high ratings to all movies, some tend to dislike all movies). Can we add one bias value to both user and movie latent factors to explain this interesting observation? How to use the bias factors inside the collab model?\n\n\n\n\n\nWhy did the upgraded model with bias get worse? (overfitting).",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 7"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson7.html#weight-decay",
    "href": "Lessons/Summaries/lesson7.html#weight-decay",
    "title": "Lesson 7",
    "section": "Weight decay",
    "text": "Weight decay\n\nWhat is weight decay and How does it help? How to understand weight decay in solving the problem of overfitting?\n\n\n\nHow to actually use weight decay in fastai code? Does fastai have a good default for collaborative filtering like CV? How does Jeremy suggest to find the appropriate wd value for your own dataset?\n\n\n\n\nWhat is regularization? What’s wrong when the weights having high values or low values? How does weight decay help balance?\nMore questions: any other rules other than Jeremy’s rule of thumb on number of latent factors, and recommendation on average rating is viable only when there are many metadata.",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 7"
    ]
  },
  {
    "objectID": "Lessons/lesson13.html",
    "href": "Lessons/lesson13.html",
    "title": "13: Backpropagation & MLP",
    "section": "",
    "text": "In this lesson, we dive into backpropagation and the creation of a simple Multi-Layer Perceptron (MLP) neural network. We start by reviewing basic neural networks and their architecture, then move on to implementing a simple MLP from scratch. We focus on understanding the chain rule and backpropagation in the context of neural networks, and demonstrate how to calculate derivatives using Python and the SimPy library.\nWe also discuss the importance of the chain rule in calculating the gradient of the mean squared error (MSE) applied to a model, and demonstrate how to use PyTorch to calculate derivatives and simplify the process by creating classes for ReLU and linear functions. We then explore the issues with floating point math and introduce the log sum exp trick to overcome these issues. Finally, we create a training loop for a simple neural network.",
    "crumbs": [
      "Part 2",
      "13: Backpropagation & MLP"
    ]
  },
  {
    "objectID": "Lessons/lesson13.html#concepts-discussed",
    "href": "Lessons/lesson13.html#concepts-discussed",
    "title": "13: Backpropagation & MLP",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nBasic neural network architecture\nMulti-Layer Perceptron (MLP) implementation\nGradients and derivatives\nChain rule and backpropagation\nPython debugger (pdb)\nPyTorch for calculating derivatives\nReLU and linear function classes\nLog sum exp trick\nlog_softmax() function and cross entropy loss\nTraining loop for a simple neural network",
    "crumbs": [
      "Part 2",
      "13: Backpropagation & MLP"
    ]
  },
  {
    "objectID": "Lessons/lesson13.html#video",
    "href": "Lessons/lesson13.html#video",
    "title": "13: Backpropagation & MLP",
    "section": "Video",
    "text": "Video",
    "crumbs": [
      "Part 2",
      "13: Backpropagation & MLP"
    ]
  },
  {
    "objectID": "Lessons/lesson13.html#lesson-resources",
    "href": "Lessons/lesson13.html#lesson-resources",
    "title": "13: Backpropagation & MLP",
    "section": "Lesson resources",
    "text": "Lesson resources\n\nDiscuss this lesson\nThe Intuitive Notion of the Chain Rule\nThe Matrix Calculus You Need For Deep Learning\nPart 1 Excel workbooks\nCalculus help topic\nSimple Neural Net Backward Pass",
    "crumbs": [
      "Part 2",
      "13: Backpropagation & MLP"
    ]
  },
  {
    "objectID": "Lessons/lesson25.html",
    "href": "Lessons/lesson25.html",
    "title": "25: Latent diffusion",
    "section": "",
    "text": "In this final lesson of the series, Johno begins by showing us how we can convert sounds into pictures, and then take advantage of what we’ve learned in this course to generate audio! He builds and demonstrates a very effective bird-song generator using this approach.\nThen Jeremy wraps up “Stable diffusion from scratch” by showing how to use the latents in a variational encoder as the “pixels” in a regular diffusion model. He also describes an intriguing new idea for students to follow up: what if you use latents for other purposes, such as a classification model? Perhaps this would open up a whole world of possibilities, such as latents-FID, latents-perceptual-loss, and new approaches to diffusion guidance!",
    "crumbs": [
      "Part 2",
      "25: Latent diffusion"
    ]
  },
  {
    "objectID": "Lessons/lesson25.html#video",
    "href": "Lessons/lesson25.html#video",
    "title": "25: Latent diffusion",
    "section": "Video",
    "text": "Video",
    "crumbs": [
      "Part 2",
      "25: Latent diffusion"
    ]
  },
  {
    "objectID": "Lessons/lesson25.html#lesson-resources",
    "href": "Lessons/lesson25.html#lesson-resources",
    "title": "25: Latent diffusion",
    "section": "Lesson resources",
    "text": "Lesson resources\n\nDiscuss this lesson\n02_diffusion for audio.pynb\nRiffusion: demo | repo\nNotebooks discussed: nb 29 | nb 30 | nb 31 | Johno’s Simple Diffusion for audio",
    "crumbs": [
      "Part 2",
      "25: Latent diffusion"
    ]
  },
  {
    "objectID": "Lessons/lesson1.html#video",
    "href": "Lessons/lesson1.html#video",
    "title": "1: Getting started",
    "section": "Video",
    "text": "Video\nClick the video below to play. Once it’s playing, you’ll see a little rectangle in the bottom-right—click it to make the video full-screen. Press Esc to remove full-screen view. Press c to turn subtitles on/off.\n\n\nThis lesson is based partly on chapter 1 of the book.",
    "crumbs": [
      "Part 1",
      "1: Getting started"
    ]
  },
  {
    "objectID": "Lessons/lesson1.html#how-to-complete-lesson-1",
    "href": "Lessons/lesson1.html#how-to-complete-lesson-1",
    "title": "1: Getting started",
    "section": "How to complete lesson 1",
    "text": "How to complete lesson 1\nEvery lesson includes lots of hands-on exercises for you to try. Most of these are run in interactive notebooks, all of which are available on Kaggle. If you don’t work through the notebooks yourself, you’re not going to get nearly as much out of this course—so that means you need to get set up on Kaggle. We have a page to help you get going with Kaggle: click here to go there now. Instead of using Kaggle, another great option is Paperspace Gradient If you don’t have a Paperspace account yet, sign up with this link to get $10 credit (and we get a credit too).\nOnce you’ve got your Kaggle account set up, you’ll need to get familiar with Jupyter Notebook, which is the platform we use for most of this course (and which most deep learning researchers and engineers use for their work). Jupyter is the most popular tool for doing data science in Python, for good reason. It is powerful, flexible, and easy to use. We think you will love it! Since the most important thing for learning deep learning is writing code and experimenting, it’s important that you have a great platform for experimenting with code. If you haven’t used it before, we’ve provided this to help you get started: Jupyter Notebook 101.\nOK, now that you have your Kaggle account and know how to use Jupyter, you’re ready to open the notebook for this lesson: here it is. For every lesson, you can find links to all notebooks used in the Resources section of the lesson web page. For instance, for lesson 1, you’ll see that section immediately below this one.\nAs well as watching the video and working through the notebooks, you should also read the relevent chapter(s) of the fast.ai book, Practical Deep Learning for Coders. Each lesson will tell you what chapter you need to read, just below the video. For this lesson, it’s chapter 1. There’s a few ways to read the book – you can buy it as a paper book or Kindle ebook, or you can read it for free as a Jupyter notebook. The whole book is written as Jupyter notebooks, so you can also execute all the code in the book yourself. To go to the interactive Jupyter version of any chapter, click The book in the left sidebar, where you’ll find a list of chapter links. You’ll also find links to read-only versions of each chapter there.",
    "crumbs": [
      "Part 1",
      "1: Getting started"
    ]
  },
  {
    "objectID": "Lessons/lesson1.html#resources",
    "href": "Lessons/lesson1.html#resources",
    "title": "1: Getting started",
    "section": "Resources",
    "text": "Resources\n\nKaggle notebooks for this lesson:\n\nIs it a bird? Creating a model from your own data\nJupyter Notebook 101\n\nThe fastai book:\n\nPublished version\nFree notebook version\nChapter 1 notebook\n\nRepo containing all lesson notebooks\nSolutions to chapter 1 questions from the book",
    "crumbs": [
      "Part 1",
      "1: Getting started"
    ]
  },
  {
    "objectID": "Lessons/lesson1.html#links",
    "href": "Lessons/lesson1.html#links",
    "title": "1: Getting started",
    "section": "Links",
    "text": "Links\nYou’ll see that fast.ai’s way of teaching is very different to what you might be used to, if you did a technical degree at university. Nearly all technical subjects at university are taught “bottom up”: start with basic foundations, and gradually work up to complete useful solutions to real world problems. But we go “top down”: start with complete useful solutions to real world problems, and gradually work down to the basic foundations. Education experts recommend this approach for more effective learning. For more information, have a look at this article that discusses the fast.ai teaching philosophy: Providing a Good Education in Deep Learning.\n\nHow to learn - highly recommended books for fast.ai students\n\nMeta Learning\nA Mathematician’s Lament by Paul Lockhart\nMaking Learning Whole by David Perkins\n\nJupyter\n\nPresentations: RISE\nBlogging: fastpages\nThe notebooks used to create the fastai library\nnbdev - the system we built to create Python libraries using Jupyter\n\nFastai: A Layered API for Deep Learning paper: Information Journal or arxiv or fast.ai\nDall-e 2 illustrations of Twitter bios\ntimm: PyTorch Image Models",
    "crumbs": [
      "Part 1",
      "1: Getting started"
    ]
  },
  {
    "objectID": "Lessons/lesson1.html#if-you-need-help",
    "href": "Lessons/lesson1.html#if-you-need-help",
    "title": "1: Getting started",
    "section": "If you need help",
    "text": "If you need help\nThere’s lot of helpful people, and helpful answers to past questions, on the fast.ai forums. There are special help topics for beginner questions, to ensure that your questions aren’t missed:\n\nHelp: Setup\nHelp: Creating a dataset, and using Gradio / Spaces\nHelp: Using Colab or Kaggle\nHelp: Python, git, bash, etc\nHelp: SGD and Neural Net foundations\nHelp: Basics of fastai, PyTorch, numpy, etc\nHelp: Beginner questions that don’t fit elsewhere",
    "crumbs": [
      "Part 1",
      "1: Getting started"
    ]
  },
  {
    "objectID": "Lessons/lesson3.html#video",
    "href": "Lessons/lesson3.html#video",
    "title": "3: Neural net foundations",
    "section": "Video",
    "text": "Video\n\n\nThis lesson is based partly on chapter 4 of the book.",
    "crumbs": [
      "Part 1",
      "3: Neural net foundations"
    ]
  },
  {
    "objectID": "Lessons/lesson3.html#resources",
    "href": "Lessons/lesson3.html#resources",
    "title": "3: Neural net foundations",
    "section": "Resources",
    "text": "Resources\n\nNotebooks for this lesson:\n\nHuggingFace Spaces Pets repository\nWhich image models are best?\nHow does a neural net really work?\n\nOther resources for the lesson\n\nTitanic spreadsheet: see the course repository\nTitanic data (training CSV) can be downloaded from Kaggle\n\nSolutions to chapter 4 questions from the book",
    "crumbs": [
      "Part 1",
      "3: Neural net foundations"
    ]
  },
  {
    "objectID": "Lessons/lesson3.html#links-from-the-lesson",
    "href": "Lessons/lesson3.html#links-from-the-lesson",
    "title": "3: Neural net foundations",
    "section": "Links from the lesson",
    "text": "Links from the lesson\n\nKnow your pet\n“Lesson 0”",
    "crumbs": [
      "Part 1",
      "3: Neural net foundations"
    ]
  },
  {
    "objectID": "Lessons/lesson21.html",
    "href": "Lessons/lesson21.html",
    "title": "21: DDIM",
    "section": "",
    "text": "In this lesson, Jeremy, Johno, and Tanishq discuss their experiments with the Fashion-MNIST dataset and the CIFAR-10 dataset, a popular dataset for image classification and generative modeling. They introduce Weights and Biases (W&B), an experiment tracking and logging tool that can help manage and visualize the progress of their experiments. The Fréchet Inception Distance (FID) metric is introduced to measure the quality of generated images, and Jeremy demonstrates how to calculate the FID using a custom Fashion-MNIST model. The lesson also covers the Fréchet Inception Distance (FID) and Kernel Inception Distance (KID) metrics for comparing image distributions.\nJeremy explores ways to make the model faster without sacrificing quality. The Denoising Diffusion Implicit Model (DDIM) is introduced as a faster alternative to DDPM, and Jeremy demonstrates how to build a custom DDIM from scratch. The lesson concludes with a discussion on the differences between DDPM and DDIM, as well as the benefits of using DDIM for rapid sampling.",
    "crumbs": [
      "Part 2",
      "21: DDIM"
    ]
  },
  {
    "objectID": "Lessons/lesson21.html#concepts-discussed",
    "href": "Lessons/lesson21.html#concepts-discussed",
    "title": "21: DDIM",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nWeights and Biases (W&B) for experiment tracking\nFréchet Inception Distance (FID) metric\nKernel Inception Distance (KID) metric\nDenoising Diffusion Implicit Model (DDIM)",
    "crumbs": [
      "Part 2",
      "21: DDIM"
    ]
  },
  {
    "objectID": "Lessons/lesson21.html#video",
    "href": "Lessons/lesson21.html#video",
    "title": "21: DDIM",
    "section": "Video",
    "text": "Video\n\n\n\nDiscuss this lesson",
    "crumbs": [
      "Part 2",
      "21: DDIM"
    ]
  },
  {
    "objectID": "Lessons/lesson15.html",
    "href": "Lessons/lesson15.html",
    "title": "15: Autoencoders",
    "section": "",
    "text": "We start with a dive into convolutional autoencoders and explore the concept of convolutions. Convolutions help neural networks understand the structure of a problem, making it easier to solve. We learn how to apply a convolution to an image using a kernel and discuss techniques like im2col, padding, and stride. We also create a CNN from scratch using a sequential model and train it on the GPU.\nWe then attempt to build an autoencoder, but face issues with speed and accuracy. To address these issues, we introduce the concept of a Learner, which allows for faster experimentation and better understanding of the model’s performance. We create a simple Learner and demonstrate its use with a multi-layer perceptron (MLP) model.\nFinally, we discuss the importance of understanding Python concepts such as try-except blocks, decorators, getattr, and debugging to reduce cognitive load while learning the framework being built.",
    "crumbs": [
      "Part 2",
      "15: Autoencoders"
    ]
  },
  {
    "objectID": "Lessons/lesson15.html#concepts-discussed",
    "href": "Lessons/lesson15.html#concepts-discussed",
    "title": "15: Autoencoders",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nConvolutional autoencoders\nConvolutions and kernels\nIm2col technique\nPadding and stride in CNNs\nReceptive field\nBuilding a CNN from scratch\nCreating a Learner for faster experimentation\nPython concepts: try-except blocks, decorators, getattr, and debugging\nCognitive load theory in learning",
    "crumbs": [
      "Part 2",
      "15: Autoencoders"
    ]
  },
  {
    "objectID": "Lessons/lesson15.html#video",
    "href": "Lessons/lesson15.html#video",
    "title": "15: Autoencoders",
    "section": "Video",
    "text": "Video",
    "crumbs": [
      "Part 2",
      "15: Autoencoders"
    ]
  },
  {
    "objectID": "Lessons/lesson15.html#lesson-resources",
    "href": "Lessons/lesson15.html#lesson-resources",
    "title": "15: Autoencoders",
    "section": "Lesson resources",
    "text": "Lesson resources\n\nDiscuss this lesson\nExcel workbooks",
    "crumbs": [
      "Part 2",
      "15: Autoencoders"
    ]
  },
  {
    "objectID": "Lessons/lesson17.html",
    "href": "Lessons/lesson17.html",
    "title": "17: Initialization/normalization",
    "section": "",
    "text": "In this lesson, we discuss the importance of weight initialization in neural networks and explore various techniques to improve training. We start by introducing changes to the miniai library and demonstrate the use of HooksCallback and ActivationStats for better visualization. We then dive into the importance of having zero mean and unit standard deviation in neural networks and introduce the Glorot (Xavier) initialization.\nWe also cover variance, standard deviation, and covariance, and their significance in understanding relationships between data points. We create a novel Generalized ReLU activation function and discuss the Layer-wise Sequential Unit Variance (LSUV) technique for initializing any neural network. We explore normalization techniques, such as Layer Normalization and Batch Normalization, and briefly mention other normalization methods like Instance Norm and Group Norm.\nFinally, we experiment with different batch sizes, learning rates, and optimizers like Accelerated SGD, RMSProp, and Adam to improve performance.",
    "crumbs": [
      "Part 2",
      "17: Initialization/normalization"
    ]
  },
  {
    "objectID": "Lessons/lesson17.html#concepts-discussed",
    "href": "Lessons/lesson17.html#concepts-discussed",
    "title": "17: Initialization/normalization",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nCallback class and TrainLearner subclass\nHooksCallback and ActivationStats\nGlorot (Xavier) initialization\nVariance, standard deviation, and covariance\nGeneral ReLU activation function\nLayer-wise Sequential Unit Variance (LSUV)\nLayer Normalization and Batch Normalization\nInstance Norm and Group Norm\nAccelerated SGD, RMSProp, and Adam optimizers\nExperimenting with batch sizes and learning rates",
    "crumbs": [
      "Part 2",
      "17: Initialization/normalization"
    ]
  },
  {
    "objectID": "Lessons/lesson17.html#video",
    "href": "Lessons/lesson17.html#video",
    "title": "17: Initialization/normalization",
    "section": "Video",
    "text": "Video",
    "crumbs": [
      "Part 2",
      "17: Initialization/normalization"
    ]
  },
  {
    "objectID": "Lessons/lesson17.html#papers-from-the-lesson",
    "href": "Lessons/lesson17.html#papers-from-the-lesson",
    "title": "17: Initialization/normalization",
    "section": "Papers from the lesson",
    "text": "Papers from the lesson\n\nDiscuss this lesson\nUnderstanding the difficulty of training deep feedforward neural networks - Xavier Glorot, Yoshua Bengio\nDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification - Kaiming He et al\nLSUV - All you need is a good init - Dmytro Mishkin, Jiri Matas\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift - Sergey Ioffe, Christian Szegedy\nLayer Normalization - Ba, Kiros, Hinton",
    "crumbs": [
      "Part 2",
      "17: Initialization/normalization"
    ]
  },
  {
    "objectID": "Lessons/lesson4.html#video",
    "href": "Lessons/lesson4.html#video",
    "title": "4: Natural Language (NLP)",
    "section": "Video",
    "text": "Video\n\n\nThis lesson is based partly on chapter 10 of the book.",
    "crumbs": [
      "Part 1",
      "4: Natural Language (NLP)"
    ]
  },
  {
    "objectID": "Lessons/lesson4.html#resources",
    "href": "Lessons/lesson4.html#resources",
    "title": "4: Natural Language (NLP)",
    "section": "Resources",
    "text": "Resources\n\nNotebook: Getting started with NLP for absolute beginners",
    "crumbs": [
      "Part 1",
      "4: Natural Language (NLP)"
    ]
  },
  {
    "objectID": "Lessons/lesson11.html",
    "href": "Lessons/lesson11.html",
    "title": "11: Matrix multiplication",
    "section": "",
    "text": "In this lesson, we discuss various techniques and experiments shared by students on the forum, such as interpolating between prompts for visually appealing transitions and improving the update process in text-to-image generation, and a novel approach to decreasing the guidance scale during image generation. We then dive into a new paper called DiffEdit, which focuses on semantic image editing using text-conditioned diffusion models. We walk through the process of reading and understanding the paper, emphasizing the importance of grasping the main idea and not getting bogged down in every detail.\nWe then embark on a deep exploration of matrix multiplication using Python, compare APL with PyTorch, and introduce the concept of Frobenius norm. We also discuss the powerful concept of broadcasting, which allows for operations between tensors of different shapes, and demonstrate its efficiency in speeding up matrix multiplication. The techniques introduced in this lesson allow us to speed up our initial Python implementation by a factor of around five million, including leveraging the GPU for massive parallelism!",
    "crumbs": [
      "Part 2",
      "11: Matrix multiplication"
    ]
  },
  {
    "objectID": "Lessons/lesson11.html#concepts-discussed",
    "href": "Lessons/lesson11.html#concepts-discussed",
    "title": "11: Matrix multiplication",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nDiffusion improvements\n\nInterpolating between prompts for visually appealing transitions\nImproving the update process in text-to-image generation\nDecreasing the guidance scale during image generation\n\nUnderstanding research papers\nMatrix multiplication using Python and Numba\nComparing APL with PyTorch\nFrobenius norm\nBroadcasting in deep learning and machine learning code",
    "crumbs": [
      "Part 2",
      "11: Matrix multiplication"
    ]
  },
  {
    "objectID": "Lessons/lesson11.html#video",
    "href": "Lessons/lesson11.html#video",
    "title": "11: Matrix multiplication",
    "section": "Video",
    "text": "Video",
    "crumbs": [
      "Part 2",
      "11: Matrix multiplication"
    ]
  },
  {
    "objectID": "Lessons/lesson11.html#lesson-resources",
    "href": "Lessons/lesson11.html#lesson-resources",
    "title": "11: Matrix multiplication",
    "section": "Lesson resources",
    "text": "Lesson resources\n\nDiscuss this lesson\nDiffEdit: Diffusion-based semantic image editing with mask guidance\nMath notation\n\nGreek letters\nAll in one mathematics cheat sheet (PDF)\nGlossary of mathematical symbols (wikipedia)\npix2tex (open source) or Mathpix (commercial)\nGreek Letters for Deep Learning - Anki deck containing fastai-related Greek letters\nDetexify Draw math symbols",
    "crumbs": [
      "Part 2",
      "11: Matrix multiplication"
    ]
  },
  {
    "objectID": "Lessons/lesson7.html#video",
    "href": "Lessons/lesson7.html#video",
    "title": "7: Collaborative filtering",
    "section": "Video",
    "text": "Video\n\n\nThis lesson is based partly on chapter 8 of the book.",
    "crumbs": [
      "Part 1",
      "7: Collaborative filtering"
    ]
  },
  {
    "objectID": "Lessons/lesson7.html#resources",
    "href": "Lessons/lesson7.html#resources",
    "title": "7: Collaborative filtering",
    "section": "Resources",
    "text": "Resources\n\nNotebooks for this lesson:\n\nRoad to the top: part 3 and part 4\nCollaborative Filtering Deep Dive\n\nSpreadsheets for this lesson:\n\nSoftmax and cross-entropy\nCollaborative filterings and embeddings\n\nThings that confused me about cross-entropy by Chris Said\nLabel Smoothing Explained using Microsoft Excel by Aman Arora",
    "crumbs": [
      "Part 1",
      "7: Collaborative filtering"
    ]
  },
  {
    "objectID": "Lessons/lesson24.html",
    "href": "Lessons/lesson24.html",
    "title": "24: Attention & transformers",
    "section": "",
    "text": "In this lesson, we wrap up our exploration of the unconditional stable diffusion model. We then implement the unconditional model, train it on fashion MNIST, and discuss the importance of time embedding. We also dive into sine and cosine embeddings, attention mechanisms, self-attention, and multi-headed attention in the context of stable diffusion. We discuss the rearrange function, transformers, and their potential use in vision tasks. Lastly, we create a conditional model by adding a label to the input of the UNet model, allowing it to generate images of a specific class.",
    "crumbs": [
      "Part 2",
      "24: Attention & transformers"
    ]
  },
  {
    "objectID": "Lessons/lesson24.html#concepts-discussed",
    "href": "Lessons/lesson24.html#concepts-discussed",
    "title": "24: Attention & transformers",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nImplementing an unconditional stable diffusion model\nTime embedding and sine/cosine embeddings\nSelf-attention and multi-headed attention\nRearrange function\nTransformers\nCreating a conditional stable diffusion model",
    "crumbs": [
      "Part 2",
      "24: Attention & transformers"
    ]
  },
  {
    "objectID": "Lessons/lesson24.html#video",
    "href": "Lessons/lesson24.html#video",
    "title": "24: Attention & transformers",
    "section": "Video",
    "text": "Video\n\n\n\nDiscuss this lesson",
    "crumbs": [
      "Part 2",
      "24: Attention & transformers"
    ]
  },
  {
    "objectID": "Lessons/lesson22.html",
    "href": "Lessons/lesson22.html",
    "title": "22: Karras et al (2022)",
    "section": "",
    "text": "Jeremy begins this lesson with a discussion of improvements to the DDPM/DDIM implementation. He explores the removal of the concept of an integral number of steps, making the process more continuous. He then delves into predicting the amount of noise in an image without passing the time step as input and modifies the DDIM step to use the predicted alpha bar for each image.\nThe focus of the lesson is to study and implement the 2022 paper by Karras et al, Elucidating the Design Space of Diffusion-Based Generative Models. The paper uses pre-conditioning to ensure that inputs and targets to the model are scaled to unit variance. The model predicts an interpolated version of the clean image and the noise, depending on the amount of noise present in the input.\nThe lesson covers various sampling techniques, such as the Euler sampler, Ancestral Euler sampler, and Heuns method. Jeremy explains the concepts behind these methods and demonstrates how they can be used to improve the sampling process. He emphasizes the importance of understanding the underlying concepts and techniques in research papers and demonstrates how these can be applied to improve model performance.",
    "crumbs": [
      "Part 2",
      "22: Karras et al (2022)"
    ]
  },
  {
    "objectID": "Lessons/lesson22.html#concepts-discussed",
    "href": "Lessons/lesson22.html#concepts-discussed",
    "title": "22: Karras et al (2022)",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nDDPM/DDIM improvements\nPredicting the amount of noise in an image\nNoise scheduling for diffusion models\nScaling input and output images\nImportance of unit variance inputs and outputs\nImplementation and performance of different samplers\n\nEuler sampler\nAncestral Euler sampler\nHeuns method\nLMS sampler",
    "crumbs": [
      "Part 2",
      "22: Karras et al (2022)"
    ]
  },
  {
    "objectID": "Lessons/lesson22.html#video",
    "href": "Lessons/lesson22.html#video",
    "title": "22: Karras et al (2022)",
    "section": "Video",
    "text": "Video\n\n\n\nDiscuss this lesson",
    "crumbs": [
      "Part 2",
      "22: Karras et al (2022)"
    ]
  },
  {
    "objectID": "Lessons/lesson2.html#video",
    "href": "Lessons/lesson2.html#video",
    "title": "2: Deployment",
    "section": "Video",
    "text": "Video\n\n\nThis lesson is based partly on chapter 2 of the book.",
    "crumbs": [
      "Part 1",
      "2: Deployment"
    ]
  },
  {
    "objectID": "Lessons/lesson2.html#resources",
    "href": "Lessons/lesson2.html#resources",
    "title": "2: Deployment",
    "section": "Resources",
    "text": "Resources\n\nNotebook—saving a basic fastai model:\n\nKaggle\nColab\n\nChapter 2 notebook\nSolutions to chapter 2 questions from the book",
    "crumbs": [
      "Part 1",
      "2: Deployment"
    ]
  },
  {
    "objectID": "Lessons/lesson2.html#links",
    "href": "Lessons/lesson2.html#links",
    "title": "2: Deployment",
    "section": "Links",
    "text": "Links\n\nGradio tutorial from @ilovescience\nHF Spaces\nInstalling a python environment\n\nfastsetup\nWindows: WSL and Terminal\n\ntinypets github / site\ntinypets fork github / site",
    "crumbs": [
      "Part 1",
      "2: Deployment"
    ]
  },
  {
    "objectID": "Lessons/lesson8.html#video",
    "href": "Lessons/lesson8.html#video",
    "title": "8: Convolutions (CNNs)",
    "section": "Video",
    "text": "Video\n\n\nThis lesson is based partly on chapter 13 of the book.",
    "crumbs": [
      "Part 1",
      "8: Convolutions (CNNs)"
    ]
  },
  {
    "objectID": "Lessons/lesson8.html#resources",
    "href": "Lessons/lesson8.html#resources",
    "title": "8: Convolutions (CNNs)",
    "section": "Resources",
    "text": "Resources\n\nNotebooks for this lesson\n\nCollaborative Filtering Deep Dive\n\nSpreadsheets for this lesson\n\nCollaborative filterings and embeddings\nConvolutions\n\nOther resources for the lesson\n\nPlease add any questions you want Jeremy to answer to the AMA thread – and upvote any there you’re interested in\nSpecial extra: Data ethics lesson\n\nSolutions to chapter 8 questionnaire from the book",
    "crumbs": [
      "Part 1",
      "8: Convolutions (CNNs)"
    ]
  },
  {
    "objectID": "Lessons/lesson12.html",
    "href": "Lessons/lesson12.html",
    "title": "12: Mean shift clustering",
    "section": "",
    "text": "In this lesson, we start by discussing the CLIP Interrogator, a Hugging Face Spaces Gradio app that generates text prompts for creating CLIP embeddings. We then dive back into matrix multiplication, using Einstein summation notation and torch.einsum to simplify code and improve performance. We explore GPU acceleration with CUDA and Numba, demonstrating how to write a kernel function for matrix multiplication and launch it on the GPU.\nNext up we exercise our tensor programming skills by implementing mean shift clustering, a technique for identifying clusters within a dataset. We create synthetic data, explain the mean shift algorithm, and introduce the Gaussian kernel for penalizing distant points. We implement the mean shift clustering algorithm using PyTorch and discuss the importance of tensor manipulation operations for efficient GPU programming.\nFinally, we optimize the mean shift algorithm using PyTorch and GPUs, demonstrating how to calculate weights, multiply matrices, and sum up points to obtain new data points. We explore the impact of changing batch sizes on performance and encourage viewers to research other clustering algorithms.\nThe lesson concludes with an introduction to calculus, focusing on derivatives and the calculus of infinitesimals.",
    "crumbs": [
      "Part 2",
      "12: Mean shift clustering"
    ]
  },
  {
    "objectID": "Lessons/lesson12.html#concepts-discussed",
    "href": "Lessons/lesson12.html#concepts-discussed",
    "title": "12: Mean shift clustering",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nCLIP Interrogator\nInverse problems\nMatrix multiplication\nEinstein summation notation and torch.einsum\nGPU acceleration and CUDA\nNumba\nMean shift clustering\nGaussian kernel\nNorms\nEuclidean distance\nCalculus\n\nDerivatives and Infinitesimals",
    "crumbs": [
      "Part 2",
      "12: Mean shift clustering"
    ]
  },
  {
    "objectID": "Lessons/lesson12.html#video",
    "href": "Lessons/lesson12.html#video",
    "title": "12: Mean shift clustering",
    "section": "Video",
    "text": "Video",
    "crumbs": [
      "Part 2",
      "12: Mean shift clustering"
    ]
  },
  {
    "objectID": "Lessons/lesson12.html#lesson-resources",
    "href": "Lessons/lesson12.html#lesson-resources",
    "title": "12: Mean shift clustering",
    "section": "Lesson resources",
    "text": "Lesson resources\n\nDiscuss this lesson\nCLIP Interrogator\nEssence of calculus (3blue1brown)",
    "crumbs": [
      "Part 2",
      "12: Mean shift clustering"
    ]
  },
  {
    "objectID": "Lessons/lesson14.html",
    "href": "Lessons/lesson14.html",
    "title": "14: Backpropagation",
    "section": "",
    "text": "In this lesson, we dive into the implementation of the chain rule in neural network training using backpropagation. We refactor our code to make it more efficient and flexible, and explore PyTorch’s nn.Module and nn.Sequential. We also create custom PyTorch modules, build our own implementation of nn.Module, and learn about optimizers, DataLoaders, and Datasets. We show how to work with Hugging Face datasets, and introduce the nbdev library.\nWe look at how to map the code from the previous lesson to the math behind backpropagation. Next, we refactor our code using PyTorch’s nn.Module, which automatically tracks layers and parameters. We also create a sequential model using nn.Sequential and demonstrate how to create custom PyTorch modules. We then introduce the concept of an optimizer, which simplifies the process of updating parameters based on gradients and learning rates. We create a custom SGD optimizer from scratch and explore PyTorch’s built-in DataLoader. We also create a proper training loop using PyTorch DataLoader.\nThroughout the lesson, we emphasize the importance of understanding the underlying code and not relying solely on other people’s code. This allows for greater flexibility and creativity in building custom solutions. We also discuss the use of **kwargs and delegates in fastcore, callbacks, and dunder methods in Python’s data model.",
    "crumbs": [
      "Part 2",
      "14: Backpropagation"
    ]
  },
  {
    "objectID": "Lessons/lesson14.html#concepts-discussed",
    "href": "Lessons/lesson14.html#concepts-discussed",
    "title": "14: Backpropagation",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nBackpropagation and the chain rule\nRefactoring code for efficiency and flexibility\nPyTorch’s nn.Module and nn.Sequential\nCreating custom PyTorch modules\nImplementing optimizers, DataLoaders, and Datasets\nWorking with Hugging Face datasets\nUsing nbdev to create Python modules from Jupyter notebooks\n**kwargs and delegates\nCallbacks and dunder methods in Python’s data model\nBuilding a proper training loop using PyTorch DataLoader",
    "crumbs": [
      "Part 2",
      "14: Backpropagation"
    ]
  },
  {
    "objectID": "Lessons/lesson14.html#video",
    "href": "Lessons/lesson14.html#video",
    "title": "14: Backpropagation",
    "section": "Video",
    "text": "Video\n\n\n\nDiscuss this lesson",
    "crumbs": [
      "Part 2",
      "14: Backpropagation"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson4.html",
    "href": "Lessons/Summaries/lesson4.html",
    "title": "Lesson 4",
    "section": "",
    "text": "New and Exciting Content\nWhy Hugging Face transformer\n\nWill we in this lecture fine-tune a pretrained NLP model with HF rather than fastai library?\nWhy use transformer rather than fastai library?\nIs Jeremy in the process of integrating transformer into fastai library?\nDoes transformer has the same layered architecture of fastai? Is it high level enough?\nWhy it is a good thing to use a reasonably high level library (not as high as fastai)?\n\nUnderstand Fine-tuning\n\nDo we have the foundations to understand the details of fine-tuning now?\nHow to understand pretrained model in terms of parameters confidence? 03:51\nIs fine-tuning trying to increase on the parameters which are not confident?\n\nULMFiT: the first fine-tuned NLP model\n\nWhere this model was first developed and taught?\nWho wrote the paper?\nWhat’s its impact?\n\nULMFiT step 1: a language model from scratch\n\nWhat is the first language model in step one?\nWhat’s the model trying to predict? What’s the dataset?\nWhy is this task so difficult? 06:10\nHow much knowledge does the model have to understand in order to predict?\nHow well can this first model predict in step one?\n\nStep 2: fine-tuned the first model on IMDB\n\nHow did Jeremy build the second language model?\nWhere did the second model start with? What was the dataset for the second model?\nWhat was the second model good at predicting?\n\nStep 3: turn a language model to classify\nLabels of language models\n\nWhat are the labels for the datasets of the first two models?\n\nTransformer models vs ULMFiT\n\nWhen did the transformers first appear?\nWhat’s transformers models are built to take advantage of?\nWhat is not transformers trying to predict? (reason in part 2)\nHow transformers modified its dataset and what does it predict? 09:41\nDoes ULMFiT and Transformers really differ much on what to predict?\nHow much different are the 3 steps between ULMFiT and Transformers?\n\nWhat a model knows\n\nWhat can lower and higher layers of parameters/weights learn? 11:08\nWhat we do to those layers of weights for transfer learning? 13:20\nZeiler and Fergus paper\n\nNLP beginner on Kaggle competition\n\nUsing a Kaggle competition to introduce NLP for beginners, isn’t it amazing!\nWhy we should take Kaggle competition more seriously? 15:06\nWhat real world tasks can NLP classification do? 15:57\n\nExamine the competition dataset\n\nWhat is inside the competition dataset?\nHow classificationish does the dataset look like?\nWhat do we predict about ‘anchor’ and ‘target’?\nWhat value to predict?\nWhy it is not really a straightforward classification?\nWhat is the use of ‘context’?\n\nModel Strategy\n\nHow to modify the dataset in order to turn a similarity problem into a classification problem?\nShould we always try to solve a problem by turning it into a problem we are familiar with?\n\nGet notebook ready\n\nWhen and how to use a GPU on Kaggle?\nWhy Jeremy recommend Paperspace over Kaggle as your workstation?\nHow easy has Jeremy made it to download Kaggle dataset and work on Paperspace or locally?\nHow to do both python and bash in the same jupyter cell?\n\nGet raw dataset into documents\n\nHow to check what inside the dataset folder?\nWhy it is important to read Competition data introduction which is often overlooked?\nHow to read a csv file with pandas? 24:30\nWhat are the key four libraries for data science in python? 24:46\nWhat is the other book besides fastbook recommended by Jeremy? 25:36\nWhy you must read it too?\nHow to access and show the dataset in dataframe? 26:39\nHow to describe the dataset? What does it tell us in general? 27:10\nWhat did the number of unique data samples mean to Jeremy at first? 27:57\nHow to create a single string based on the model strategy? 28:26\nHow to refer to a column of a dataframe in reading and writing a column data?\n\nTokenization: Intro\n\nHow to turn strings/documents into numbers for neuralnet?\nDo we split the string into words first?\nWhat’s the problem with the Chinese language on words?\nWhat are vocabularies compared with splitted words?\nWhat to do with the vocabulary?\nWhy we want the vocabulary to be concise not too big?\nWhat nowadays people prefer rather than words to be included in vocab?\n\nSubwords tokenization by Transformer\n\nHow to turn our dataframe into Hugging Face Dataset?\nWhat does HF Dataset look like?\nWhat is tokenization? What does it do?\nWhy should we choose a pretrained model before tokenization?\nWhy must we use the model’s vocab instead of making our own?\nHow similar is HF model hub to TIMM? 33:10\nWhat Jeremy’s advice on how to use HF model hub?\nAre there some models generally good for most of practical problems? 34:17\nWhen did NLP models start to be actually very useful? 34:35\nWhy we don’t know much about those models which potentially are good for most of things?\nWhy should we choose a small model to start with?\nHow to get the tokens, vocabs and related info of the pretrained model? 36:04\nHow to tokenize a sentence by the model’s style?\nAfter a document is splitted into a list of vocab, do we turn the list of vocab into a list of numbers? Numericalization 38:30\nCan you get a sense of what subword vs word is from the examples of tokenization\nHow to tokenize all the documents with parallel computing? 38:50\nGiven the input column is the document, what’s inside the input_id column?\n\nSpecial treatment to build input?\n\nDo we need to follow some special treatment when building a document or an input from dataset?\nWhat about when the document is very long?\n\nStart ULMFiT on large documents\n\nWhat ULMFiT is best at doing?\nWhy ULMFiT can work on large documents fast and without that much GPU?\nHow large is large for a document?\n\nSome obscure documentations of Transformer library\nThe most important idea in ML\n\nIs it the idea of having separate training, testing, validation datasets?\n\nUnderfitting vs Overfitting\n\nHow to create a function to plot a polynomial function with a degree variable?\nWhat are 1st/2nd/3rd degree polynomial?\nWhat does Jeremy think of sklearn? When to use it? 47:37\nWhat is underfitting? Why a too-simple model is a problem or is systematically biased? 48:12\nWhat is overfitting? What does an overfit look like? 48:58\nWhat is the cause of overfitting?\nIt is easy to spot underfitting, but how to filter an overfitting from the function we want?\n\nValidation: avoid overfitting on training set\n\nHow to get a validation dataset and use it?\nWhy you need to be careful when use other libraries other than fastai?\n\nHow and Why to create a good validation set\n\nDid you know simply random 20% of dataset as a validation set is not good enough?\nFor example, shouldn’t you select validation dataset so that your model can predict the future rather than the past?\nWhy is Kaggle competition a great and real-world way to appreciate using validation set to avoid overfitting?\nHow validation set can help avoid overfitting in 2 Kaggle competition on real world problems? 54:44\nWatch out when touching cross-validation 56:03\nWhy should you be careful when simply using library-ready tools of selecting validation set randomly?\nValidation post by Rachel\n\nTest set: avoid overfitting on validation set\n\nWhat is a test set for?\nWhy need it when we have a validation set?\nWhen or how can you overfit on a validation set? or\nWhy is validation set not enough to overcome model overfitting?\nWhy Kaggle prepares two test sets? or\nWhy Kaggle thinks that two test sets are enough to filter overfitting models\n\nMetrics functions vs Loss functions\n\nHow we use validation set to check on the performance of model?\nWill Kaggle competition choose the metrics for you?\nShould the metrics be our loss function?\nWhat kind of functions you should use as loss function? (bumpy vs smooth)\nSo, always be aware: the loss your model tries to beat may not be the same function to rate your model\nWhy one metric is always not enough and can cause much problem?\n\nMetrics: you can’t feel it from math\n\nWhat is Pearson correlation (r) and how to interpret it?\nWhich can teach you how r behave, its math function or its performance on datasets?\nShould we a plot with a 1000 random data point or a plot with the entire a million data points?\nHow to get correlation coefficient for every variable to every other variable? 1:06:27\nHow to read the correlation coefficient matrix?\nHow to get a single correlation coefficient between two things?\nHow to tell how good is a correlation coefficient number? 1:07:45\nWhat are the things to spot? (tendency line, variation around the line, outliers)\nHow to create transparency on the plot?\nHow can we tell from another example that r is very sensitive to outliers? 1:09:47\nHow much can removing or mess up a few outliers really affect your scores on r? or\nWhy do you have to be careful with every row of data when dealing with r?\nCan we know how good is r = 0.34 or r = -0.2 without a plot?\nDon’t forget to get the data format right for HF\n\nHF train-validation split\n\nHow to do the random split with HF?\nWill Jeremy talk about proper split in another notebook?\n\nTraining a model\n\nWhat to use for training a model in HF?\nWhat is batch and batch size?\nHow large should a batch size be?\nHow to find a good learning rate? (details in a future lecture)\nWhere to prepare all the training arguments in HF library?\nWhich type of tasks do we use for picking the model?\nHow to create the learner or trainer after model?\nHow to train?\nWhy is the result on metrics so good right from the first epoch?\n\nDealing with outliers\n\nShould we get a second analysis for the outliers rather than simply removing them?\nWhat outliers really are in the real world?\nDoesn’t outliers usually tell us a lot of surprisingly useful info than in the limiting statistical sense?\nWhat is Jeremy’s advice on outliers?\n\nPredict and submit\n\nHow to do prediction with HF?\nShould we always check the prediction output as well as the test set input?\nWhat is the common problem with the output? (proper solution may be in the next lecture)\nWhat is the easy solution?\nHow to submit your answer to Kaggle?\n\nHuge opportunities in research and business\nMisuses of NLP\n\nCan NLP chatbots can create 99% of online chat which almost non-distinguishable from real humans?\nCan GTP-3 create even longer and more sophisticated prose which is even more human-like?\nHow machined generated public opinions can influence public policies or laws?\n\nIssues on num_labels in HF library",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 4"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson2.html",
    "href": "Lessons/Summaries/lesson2.html",
    "title": "Lesson 2",
    "section": "",
    "text": "Daniel 深度碎片 on forums.fast.ai has been kind enough to create summaries, in the form of a list of questions, of every lesson. You can use these summaries to remind yourself what you learned in each lesson, or to preview a lesson before you watch it. Here’s the lesson 2 summary:\n\nNew exciting content to come\n\nCan there be substantial new content given we have already 4 versions and a book?\n\nWays of reading the book\n\nHow many channels available for us to read the book? (physical, github, colab and others)\n\nExtra sweets from the book\n\nAre there interesting materials/stories covered by the book not the lecture?\nWhere can you find questionnaires and quizzes of the lectures?\n\naiquizzes.com\n\nWhere can you get more quizzes of fastai and memorize them forever?\n\nIntroducing the forum\n\nHow to make the most out of fastai forum?\n\nStudents’ works after week 1\nA Wow moment\n\nWill we learn to put model in production today?\n\nFind a problem and some data\n\nWhat is the first step before building a model?\n\nAccess to the magics of Jupyter notebook\n\nDo you want to navigate the notebook with a TOC?\nHow about collapsable sections?\nHow about moving between start and end of sections fast?\nHow to install jupyter extensions\n\nDownload and clean your data\n\nWhy use ggd rather than bing for searching and downloading images?\nHow to clean/remove broken images?\n\nGet to docs quickly\n\nHow to get basic info, source code, full docs on fastai codes quickly?\n\nResize your data before training\n\nHow can you specify the resize options to your data?\nWhy should we always use RandomResizedCrop and aug_transforms together?\nHow RandomResizedCrop and aug_transforms differ?\n\nData images instantly transformed not copied\n\nWhen resized, are we making many copies of the image?\n\nMore epochs for fancy resize\n\nHow many epochs do we usually go when using RandomResizedCrop and aug_transforms?\n\nConfusion matrix: where do models get wrong the most?\n\nHow to create confusion matrix on your model performance?\nWhen to use confusion matrix? (category)-practice\nHow to interpret confusion matrix?\nWhat is the most obvious thing does it tell us?\nHow hard is it to tell grizzly and black bears apart?\n\nCheck out images with worse predictions\n\nDo plot_top_losses give us the images with highest losses?\nAre those images merely ones the model made confidently wrong prediction?-practice\nDo those images include ones that the model made right prediction unconfidently?\nWhat does looking at those high loss images help? (get expert examination or simple data cleaning)\n\nWhat if you want to clean the data a little\n\nHow to display and make cleaning choices on each of those top loss images in each data folder?-practice\nWithout expert knowledge on telling apart grizzly and black bears, at least we can clean images which mess up teddy bears.\n\nMyth breaker: train model and then clean data\n\nHow can training the model help us see the problem of dataset?-practice\nWon’t we have more ideas to improve the dataset once we spot the problems of the dataset?\n\nTurn off GPU when not using\n\nHow to use GPU RAM locally without much trouble?\n\nWatch first, then watch and code along\n\nWhat is the preferred way of lecture watching and coding by majority of students?\n\nA Gradio + hugging face tutorial\nGit and Github desk\n\nIs Github desk a less cool but easier and more robust way to version control than git?\n\nTerminal for windows\n\nHow to set up terminal for windows?\nWhy Jeremy prefer windows than mac?\n\nGet started with Hugging Face Spaces\n\ngo to huggingface.co/spaces and create a new space\n\nGet the default App up and running\n\nHow to use git to download your space folder?\nHow to open vscode to add app.py file?\nHow to use vscode to push your space folder up to hugging face spaces online?\nthen go back to your space on Hugging Face to see the app running\n\nTrain and download your model\n\nWhere is the model we are going to train and download from Kaggle notebook?\nHow to export your model after trained it on Kaggle?\nWhere do you download the model?\nHow to open a folder in terminal? open .\nMake sure the model is downloaded into its own Hugging Face Space folder\n\nPredict with loaded model\n\nHow to load downloaded model to make prediction?\nHow to make prediction with the loaded model?\nHow to export selected cells of a jupyter notebook into a python file?\nHow to see how long a code runs in a jupyter cell?\n\nTurn your model into Gradio App locally\n\nHow to prepare your prediction result into a form gradio prefers? #code\nHow to build a gradio interface for your model?\nHow to launch your app with the model locally?\nNot in video: run the code on Kaggle in cloud\n\nPush this app onto Hugging Face Spaces\n\nMake sure to create a new space first, e.g., testing\nHow to turn the notebook into a python script?\nHow to push the folder up to github and run app in cloud?\nNot in Video: if stuck, check out Tanishq tutorial-shooting\n\nHow many epochs are ideal for fine tuning?\nHow to save model from colab?\nHow to install fastai properly\n\nHow to download github/fastai/fastsetup using git? git clone https://github.com/fastai/fastsetup.git\nHow to download and install mamba? ./setup_conda.sh\nNot in Video: problem of running ./setup_conda.sh\nHow to download and install fastai? mamba install -c fastchan fastai\nHow to install nbdev? mamba install -c fastchan nbdev\nHow to start to use jupyter notebook? jupyter notebook --no-browser\nNot in Video: other problem related to xcode\n\nThe workflow summary\nHuggingFace API + gradio + Javascript = real APP\nHow easy does HuggingFace API work\nHow easy to to get started with JS + HF API + gradio\nApp example of having multiple inputs and outputs\nApp example of combining two models\nHow to turn your model into your own web App with fastpages\nHow to fork a public fastpages for your own use",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 2"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson8.html",
    "href": "Lessons/Summaries/lesson8.html",
    "title": "Lesson 8",
    "section": "",
    "text": "What will part 2 feel like? a lot deeper technically? Able to read and implement research papers? Models involve real life situations?\nReview build a neuralnet from scratch. How Pytorch create a neuralnet effortlessly? How Pytorch keep track of model weights through Module? How does Module store weights with nn.Parameter? How to check weights from the model using parameters()?\n\n\n\nYou can build a layer in Module with nn.Linear without nn.Parameter and Pytorch can read weights from it too.\n\n\n\nHow to create the Embedding function and the entire DotProductBias with pytorch using create_params from scratch? After it’s trained, the trained movie_bias can be checked. You can check the shape of the bias by model.movie_bias.shape\n\n\n\n\n\nQuestions: What does Tensor.normal_ do?",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 8"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson8.html#building-embeddings-from-scratch",
    "href": "Lessons/Summaries/lesson8.html#building-embeddings-from-scratch",
    "title": "Lesson 8",
    "section": "",
    "text": "What will part 2 feel like? a lot deeper technically? Able to read and implement research papers? Models involve real life situations?\nReview build a neuralnet from scratch. How Pytorch create a neuralnet effortlessly? How Pytorch keep track of model weights through Module? How does Module store weights with nn.Parameter? How to check weights from the model using parameters()?\n\n\n\nYou can build a layer in Module with nn.Linear without nn.Parameter and Pytorch can read weights from it too.\n\n\n\nHow to create the Embedding function and the entire DotProductBias with pytorch using create_params from scratch? After it’s trained, the trained movie_bias can be checked. You can check the shape of the bias by model.movie_bias.shape\n\n\n\n\n\nQuestions: What does Tensor.normal_ do?",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 8"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson8.html#interpretation-of-embeddings",
    "href": "Lessons/Summaries/lesson8.html#interpretation-of-embeddings",
    "title": "Lesson 8",
    "section": "Interpretation of embeddings",
    "text": "Interpretation of embeddings\n\nAfter training, what can the movie_bias tell us about each and all the movies? What does having a low bias mean for a movie? What does having a high bias mean for a movie? Can user_bias tell us which user just loves movies even the crapy ones? This is visualizing movie_bias\n\n\n\nWhat can we interpret or do about the huge matrix with shape (num_users, 50)? How to shrink the 50 latent factors into just 3 most important factors with pca?\n\n\n\nHow to interpret the PCA chart of movies rated with only just two PCA factors of out 3 compressed by 50 factors? How the taste or style of the movies are condensed into two factors and displayed and defined by the location of the two dimensional chart? This is visualizing movie_factors or embeddings.\n\n\n\nHow fastai makes all the work above easier with just one line of code?\n\n\n\nHow fastai construct everything under the hood of collab_learner?\n\n\n\n\nQuestions: is PCA useful in other applications? Where to find more of PCA? Why should you take Rachel’s Computational Linear Algebra?\nHow to use Embedding distance to find out movie similarities?\n\n\n\nGo to read the fastbook for boostrapping a collaborative filtering model",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 8"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson8.html#deep-learning-for-collaborative-filtering",
    "href": "Lessons/Summaries/lesson8.html#deep-learning-for-collaborative-filtering",
    "title": "Lesson 8",
    "section": "Deep learning for collaborative filtering",
    "text": "Deep learning for collaborative filtering\n\nHow to do collaborative filtering with deep learning instead of matrix completion with dot product above? How to apply the easist neuralnet model architecture onto this collaborative filtering case?\n\n\n\nHow does fastai use rules of thumb to recommend the number of latent factors for users and movies?\n\n\n\nHow does fastai use deep learning to build collaborative filtering model in two ways?\n\n\nWhy the deep learning versions are not as good as DotProduct version? Is it because DotProduct is more tailored to the problem? How do companies combine both versions to do collaborative filtering? When you have lots of metadata, should you apply deep learning to it? How would you use metadata in the model? - Questions: Can a smaller number of users and movies overwhelm everybody else? e.g., a small group of anime enthusiasts watch a lot of anime movies and give super high ratings. Details of how to deal with them won’t be discussed here - How to apply embedding matrix into NLP model through a spreadsheet demo? What’s the essense of neuralnet?\n\n\nHow to apply embeddings to tabular dataset and models? How to understand TabularModel and tabular_learner source?\nWhat’s going on inside a neuralnet through a shop sale prediction kaggle competition and a paper published based on it?",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 8"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson8.html#convolutions",
    "href": "Lessons/Summaries/lesson8.html#convolutions",
    "title": "Lesson 8",
    "section": "Convolutions",
    "text": "Convolutions\n\nSo far we have looked at what goes in as inputs and what goes out of a model as outputs. We have also looked at the middle as matrix multiplication. What are convolution (a particular kind of matrix multiplication in the middel)? How is it be very useful to CV? Why MNIST is one of the most famous CV dataset? How does Jeremy apply what Fergus and Zeiler’s paper onto MNIST using excel and convolution?\nHow to understand convolution? What does a filter do and How does it help to detect horizontal and vertical edges? How to determine the size of the filter or kernel (3x3, or 5x5, or any)? conv1 means the first convolutional layer\nmoving onto the second convolutional layer. Two filters give us two channels on the first convolutional layer. On the second convolutional layer, we create one 3D matrix filter which has two matrix filters to filter/process the two channels out of the first conv layer, and condense the value. And we can also create a second channle for the 2nd conv layer using another 3D filter.\nHow to determine the output and use SGD to train the model and optimize the filters?\nWhat is maxpooling? What’s the problem of maxpooling? How much data do we lose? Why it is a good thing? What is a dense layer and what does it do?\nHow we do convolution slightly differently today? What is stride-two convolution and how does it work? (no more maxpooling) Then we do a lot of stride-two convolutions until the size shrinked to 7x7 and then do a average_pooling (no more dense layer). What does the 7x7 grid and take an average mean? What is the problem of such approach? When is the good time to use maxpool instead? How fastai made it easy for us to try both pooling by inventing a technique called concat_pooling to maxpool and average_pool and concat them together?\nHow to understand convolution in terms of matrix multiplications?\nWhat is dropout and how to understand it using excel? What is droput mask? What’s its effect visually on excel? How to understand dropout as data augmentation for the activations? How does it help avoid overfitting? What’s the story of dropout and academia?\nWhy Jeremy not spend much time on activation functions? We have seen many functions on metrics, loss and activations.\nWhat to do next before fastai part2? What Radek’s book meta learning is about? What are the things to do in Write, Help, Gather and Build?\na fastai community member published mish activation used by many state of art models.",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 8"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson8.html#jeremy-ama",
    "href": "Lessons/Summaries/lesson8.html#jeremy-ama",
    "title": "Lesson 8",
    "section": "Jeremy AMA",
    "text": "Jeremy AMA\n\nHow to keep up? To keep up by focusing in subfield of deep learning and focusing on things that don’t change much as the foundations of fastai have not changed much from 5 years ago. Everything else is just tweaks.\nWill huge dataset and GPU computation replace us with small dataset and one gpu? There is always smarter ways of doing things, eg. Fastai team trained on imagenet on standard GPU faster than all companies with huge amount of GPUs. Pick areas of different domains which smaller models can beat the state of the art.\nHow Jeremy to teach kids math? all kids can learn algebra with dragonbox5+. Great, Jeremy promised to talk more about teaching kids some point later.\nPlans for walkthrus\nHow to turn a model into business? Great news, Jeremy plans to build a course on this! What is the start of a business? What is the first step? How to gradually figure out whether your idea has a real need from people?\nHow Jeremy stay so efficient at working? Finish something nicely, tenacity",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 8"
    ]
  },
  {
    "objectID": "Lessons/Summaries/lesson6.html",
    "href": "Lessons/Summaries/lesson6.html",
    "title": "Lesson 6",
    "section": "",
    "text": "Review: lecture 5 and OneR from scratch\nHow to build a TwoR model manually (towards a decision tree)?\nHow to create a decision tree with 4 leaves and draw the graph? How to interpret the graph? Why decision tree is loved when doing explorative analysis?\nWhat is gini in terms of how good a split is? What is its source code? How to think of gini in terms of the probability of grab the same item in multiple times?\nWhy the mean_absolute_error of the decision tree is actually worse than the OneR version?\nHow to build a decision tree with 50 leaves? How well does it work?\nHow to make prediction and prepare a csv to upload to the kaggle leaderboard? We should start with a base model like this and improve the model everyday\nHow decision tree frees you from dummy variables, taking log of the fare, thinking of the outliers and long tail of distributions? Jeremy always use decision trees to create a base line model, which is hard to mess up. How decision trees turn categories like embarked into strings and then numbers to sort? How many levels does Jeremy usually use decision tree with?\nHow to build many unbiased and uncorrelated models (or decision trees) for bagging? This approach to build all these models is called random forest\nHow to create a random forest with 100 decision trees? How to make them random trees? How to average the predictions of all the trees and submit to kaggle?\nWhat does feature-important (a favorite of Jeremy) do? Does it care about distribution or numeric vs categorical? How does Jeremy use it? and there is an amazing story of Jeremy and feature-important for credit default problem\nDoes increasing num of trees always increase accuracy? Yes with tiny bumps. Does the increase of return decrease? The more trees you have, the longer the inference time, but you can speed it up by good code. Does Jeremy often use more than 100 trees? chp 8 of fastbook\nWhat is OOB (out of bag error)? How does bagging get away from not having a validation set given each tree only use 75%of dataset for training? Does sklearn make using OOB easy?\nWhat is bagging compared with random forest? Is random forest just bagging (meta model) applied to tabular data with lots of decision trees? Can we bag not just a lot of decision trees (into random forest) but also bag a lot of neuralnets? Will we (fastai people) do it, given most people don’t do it?\nWhat are the insights or model interpretations can be given to us by Random Forest?\nHow does the bagging help us find out how confident we are about the prediction of a row of tabular data?\nAfter you find out those important features, what to do about those less important columns or features of the tabular dataset?\nCheck out the book section on removing the redundant features\nWhat does Partial dependence do? How each column/feature is related to dependent variable? Is this one particular to Random Forest? Why calculating partial dependence is not as easy as it sounds? How does partial dependence work behind the scene? Can we do more than one feature partial dependence at one time?\nCan you explain why a particular prediction is made? Can tree interpreter give us feature importance (or the path from root to leaf) of the model prediction of a row of data?\nWould you delete a tree which does not perform well? No, doing so would bias your bagging.\nWill bagging of bags do better than one bag? No, average of averages is still average\nWhat does Jeremy think of Random forest’s feature importance vs other model explanability techniques? When to use feature importance and other explanability techniques?\ntabular section is on chp 9. Can you overfit a random forest? No, more trees make it more accurate, but not enough trees with deep levels can make your random forest overfit.\nCan you confuse random forest by adding lots of noise columns/features?\nWhat you don’t need to worry about with random forest? interaction in logistic regression, normalization\nWhat is gradient boosting? How does boosting work? Are bagging and boosting both meta models which can be applied to decision trees? Random forest vs Gradient boosting trees. Can Gradient boosting overfit given it is more accurate? What’s Jeremy’s take on random forest vs gradient boost?\nIntroducing walkthrus on paddy competition and what is so thrilling about it\nWhat is the basic process extracted from the walkthrus?\nWhat does fastkaggle do for us? How to install and update it? Can it download kaggle data for us regardless whether you are on kaggle or not?\nThere are so much benefits we can get by keep working on kaggle competitions, such as forcing you to face the truth and stopping lying to yourself about how good your model is, etc.\nWhat are the two things we should focus on? a good validation set and how to iterate in a minute. Why it is so important to iterate (telling it with a story)?\nWhen does Jeremy use seed=42 and when not?\nDo recognise how pytorch and PILImage describe the shape of a tensor/image? Pytorch (640 rows x 480 columns ) vs PILImage( 480 columns x 640 rows)\nDoes it take a lot of compute to figure out the shape or size of an image? How does Jeremy’s fastcore.parallel help to figure out the sizes of all images much faster?\nWhat’s the easist thing to do with the images? What does item_tfms=Resize(480, method='squish' do? What does batch_tfms=aug_transforms(size=128, scale=0.75 do? can we use dls.show_batch(max_n=6) for any kind of data?\nWhy Jeremy usually build a model very early on and choose the ones which can iterate fast?\nWhat is the project Jeremy and Thomas created to find out the best models for fine-tuning? How many different architectures have been examined? How different the two datasets they used?\nWhat are the criteria for evaluating the models? How they are compared? Which model architecture did Jeremy choose for his first model and why? What does Jeremy think of studying the structure of model architectures like resnet26?\nHow did Jeremy create his first model? How does Jeremy use lr_find to pick a more appropriate learning rate? How fast is Jeremy’s first model? Why Jeremy wants it this way?\nShould we submit as soon as we can? How do we check out the submit format first? How should we build a dataloader for test set? How do we predict all the test set and return a list of indices pointing to the most probable disease type? How do we create a dictionary with dls.vocab and use pandas map function to map indices to disease type strings? How to put our final processed result into a dataframe and save them into a csv file and check the results from the csv file in terminal?\nHow do we even make submitting to kaggle a fast automation?\na base line model which iterates fast and trained within a minute gets us to top 80% or bottom 20% is not bad, and is a good starting point.\nHow to even automate the process of sharing kaggle notebooks? and Why would you publish your notebooks on kaggle (or why this is very beneficial)?\nHow does Jeremy iterate models with notebooks (bon local and kaggle) in a real simple but practically effective style?\nWhat does Jeremy think of AutoML? How does Jeremy approach hyperparameter optimization? How Jeremy found out squish is always better than cropping in most cases without grid search? How Jeremy find a good learning rate fast without grid search?\nWhat is Jeremy’s rule of thumb? computer vision problem uses deep learning models, random forest (not bothered by GBM) for tabular dataset\nWhy the first model run so slow on Kaggle GPUs? How to make our model/notebook run faster on Kaggle gpus and cpus? How to first resize (down-size) all training data and put them into a different folder? How much faster did Jeremy get after this?\nHow badly the first model utilizes Kaggle GPU? Did Kaggle 2 CPUs get exhausted?\nHow did Jeremy pick the second model architecture for the second iteration?\nHow much better can a new novel architecture improve the accuracy versus the first model (resnet26)?\nWhy should we move from the era of resnet onto the new era of convnext? How to pick the appropriate models from the convnext family for our iterations?\nHow to iterate the model with different settings fast by putting everything into a single function train? How to quickly try resizing with random cropping without squish? What did Jeremy find out about this model iteration?\nHow to iterate the model with padding? What’s special about padding versus cropping versus squish? What’s its downsides? and how well did this iteration do?\nWhat does our data augmentation do to images? How to understand Test-time augmentation (tta) in terms of mini-bagging? How easy fastai makes tta work? tta should work better but in this particular kaggle run, it didn’t. Jeremy said will come back to this next time.\nHow to iterate the model with larger images and longer epochs? How much better did this iteration get us? Up to this point, the mechanism behind all the iterations above is universal to all different problems\nHow does the pandas indexing make mapping from indices to vocab string super fast? and submit to kaggle the usual way?\nDo we always do data augmentation for images? What data-augmentation does tta use?\nWhy does Jeremy use different aspect ratios during different iterations? What are the better things Jeremy has been experimenting?\nWhy Jeremy didn’t create images more image-like，but instead using simple padding (i.e., black stripes)?",
    "crumbs": [
      "Part 1",
      "Summaries",
      "Lesson 6"
    ]
  },
  {
    "objectID": "Lessons/lesson19.html",
    "href": "Lessons/lesson19.html",
    "title": "19: DDPM and Dropout",
    "section": "",
    "text": "In this lesson, Jeremy introduces Dropout, a technique for improving model performance, and with special guests Tanishq and Johno he discusses Denoising Diffusion Probabilistic Models (DDPM), the underlying foundational approach for diffusion models. The lesson covers the forward and reverse processes involved in DDPM, as well as the implementation of a noise predicting model using a neural network. The team also demonstrate an alternative approach to the implementation and discuss ways to improve training speed.",
    "crumbs": [
      "Part 2",
      "19: DDPM and Dropout"
    ]
  },
  {
    "objectID": "Lessons/lesson19.html#concepts-discussed",
    "href": "Lessons/lesson19.html#concepts-discussed",
    "title": "19: DDPM and Dropout",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nDropout technique for improving model performance\nTest time dropout callback for measuring model confidence\nDenoising Diffusion Probabilistic Models (DDPM) for generative modeling\nForward and reverse processes in DDPM\nImplementing a noise predicting model using a neural network\nTraining loop and loss function calculation in DDPM\nVisualizing noisy images at different timesteps\nAlternative noise schedules for improved DDPM performance\nInheriting from Callback and UNet2DModel for an alternative implementation\nExperimenting with initialization techniques and optimizers\nIntroduction to mixed precision for faster training",
    "crumbs": [
      "Part 2",
      "19: DDPM and Dropout"
    ]
  },
  {
    "objectID": "Lessons/lesson19.html#video",
    "href": "Lessons/lesson19.html#video",
    "title": "19: DDPM and Dropout",
    "section": "Video",
    "text": "Video\n\n\n\nDiscuss this lesson",
    "crumbs": [
      "Part 2",
      "19: DDPM and Dropout"
    ]
  },
  {
    "objectID": "Lessons/lesson23.html",
    "href": "Lessons/lesson23.html",
    "title": "23: Super-resolution",
    "section": "",
    "text": "In this lesson, we work with Tiny Imagenet to create a super-resolution U-Net model, discussing dataset creation, preprocessing, and data augmentation. The goal of super-resolution is to scale up a low-resolution image to a higher resolution. We train the model using AdamW optimizer and mixed precision, achieving an accuracy of nearly 60%. We also explore the potential for improvement by examining the results of other models on Tiny Imagenet from the Papers with Code website.\nWe discuss the limitations of using a convolutional neural network for image super-resolution and introduce the concept of U-net, a more efficient architecture for this task. We implement perceptual loss, which involves comparing the features of the output image and the target image at an intermediate layer of a pre-trained classifier model. After training the U-net model with the new loss function, the output images are less blurry and more similar to the target images.\nFinally, we discuss the challenges of comparing different models and their outputs. We demonstrate how perceptual loss has improved the results significantly, but also note that there isn’t a clear metric to use for comparison. We then move on to gradually unfreezing pre-trained networks, a favorite trick at fast.ai. We copy the weights from the pre-trained model into our model and train it for one epoch with frozen weights for the down path. This results in a significant improvement in loss.",
    "crumbs": [
      "Part 2",
      "23: Super-resolution"
    ]
  },
  {
    "objectID": "Lessons/lesson23.html#concepts-discussed",
    "href": "Lessons/lesson23.html#concepts-discussed",
    "title": "23: Super-resolution",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nTiny Imagenet dataset\nCreating a super-resolution U-Net model\nData preprocessing and augmentation\nPerceptual loss\nGradually unfreezing pre-trained networks\nExperimenting with cross connections in Unet",
    "crumbs": [
      "Part 2",
      "23: Super-resolution"
    ]
  },
  {
    "objectID": "Lessons/lesson23.html#video",
    "href": "Lessons/lesson23.html#video",
    "title": "23: Super-resolution",
    "section": "Video",
    "text": "Video",
    "crumbs": [
      "Part 2",
      "23: Super-resolution"
    ]
  },
  {
    "objectID": "Lessons/lesson23.html#lesson-resources",
    "href": "Lessons/lesson23.html#lesson-resources",
    "title": "23: Super-resolution",
    "section": "Lesson resources",
    "text": "Lesson resources\n\nDiscuss this lesson\nTrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation\nIdentity Mappings in Deep Residual Networks",
    "crumbs": [
      "Part 2",
      "23: Super-resolution"
    ]
  },
  {
    "objectID": "Lessons/lesson6.html#video",
    "href": "Lessons/lesson6.html#video",
    "title": "6: Random forests",
    "section": "Video",
    "text": "Video\n\n\nThis lesson is based partly on chapter 9 of the book.",
    "crumbs": [
      "Part 1",
      "6: Random forests"
    ]
  },
  {
    "objectID": "Lessons/lesson6.html#lesson-notebooks",
    "href": "Lessons/lesson6.html#lesson-notebooks",
    "title": "6: Random forests",
    "section": "Lesson notebooks",
    "text": "Lesson notebooks\n\nHow random forests really work\nRoad to the top, part 1",
    "crumbs": [
      "Part 1",
      "6: Random forests"
    ]
  },
  {
    "objectID": "Lessons/lesson6.html#links-from-the-lesson",
    "href": "Lessons/lesson6.html#links-from-the-lesson",
    "title": "6: Random forests",
    "section": "Links from the lesson",
    "text": "Links from the lesson\n\nHow to explain Gradient Boosting\n“Statistical Modeling: The Two Cultures” by Leo Breiman",
    "crumbs": [
      "Part 1",
      "6: Random forests"
    ]
  },
  {
    "objectID": "Lessons/lesson16.html",
    "href": "Lessons/lesson16.html",
    "title": "16: The Learner framework",
    "section": "",
    "text": "In Lesson 16, we dive into building a flexible training framework called the learner. We start with a basic callbacks Learner, which is an intermediate step towards the flexible learner. We introduce callbacks, which are functions or classes called at specific points during the training process, and demonstrate the creation of a simple callback. We also introduce the concept of CancelFitException, CancelEpochException, and CancelBatchException.\nNext, we explore metrics and create a MetricsCB callback to print out metrics during training. We introduce the torcheval library and create a DeviceCB callback to handle moving the model and data to the appropriate device. We refactor the code using a context manager to simplify the code and make it easier to maintain and add callbacks in the future.\nWe then focus on looking inside the models to diagnose and fix problems during training. We introduce a set_seed function and train a model with a high learning rate of 0.6 to test the stability of the training. Finally, we discuss analyzing the training process by looking at the mean and standard deviation of each layer’s activations, using PyTorch hooks and creating histograms of the activations.",
    "crumbs": [
      "Part 2",
      "16: The Learner framework"
    ]
  },
  {
    "objectID": "Lessons/lesson16.html#concepts-discussed",
    "href": "Lessons/lesson16.html#concepts-discussed",
    "title": "16: The Learner framework",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nBuilding a flexible training framework\nBasic Callbacks Learner\nCallbacks and exceptions (CancelFitException, CancelEpochException, CancelBatchException)\nMetrics and MetricsCB callback\ntorcheval library\nDeviceCB callback\nRefactoring code with context managers\nset_seed function\nAnalyzing the training process\nPyTorch hooks\nHistograms of activations",
    "crumbs": [
      "Part 2",
      "16: The Learner framework"
    ]
  },
  {
    "objectID": "Lessons/lesson16.html#video",
    "href": "Lessons/lesson16.html#video",
    "title": "16: The Learner framework",
    "section": "Video",
    "text": "Video",
    "crumbs": [
      "Part 2",
      "16: The Learner framework"
    ]
  },
  {
    "objectID": "Lessons/lesson16.html#lesson-resources",
    "href": "Lessons/lesson16.html#lesson-resources",
    "title": "16: The Learner framework",
    "section": "Lesson resources",
    "text": "Lesson resources\n\nDiscuss this lesson\nCyclical Learning Rates for Training Neural Networks - Leslie Smith\nA disciplined approach to neural network hyper-parameters: Part 1 – learning rate, batch size, momentum, and weight decay - Leslie Smith\nMethods for Automating Learning Rate Finders - Zach Mueller",
    "crumbs": [
      "Part 2",
      "16: The Learner framework"
    ]
  },
  {
    "objectID": "Lessons/lesson9.html",
    "href": "Lessons/lesson9.html",
    "title": "9: Stable Diffusion",
    "section": "",
    "text": "Here’s what you need to know to complete this course:\n\nThe lesson is presented as a video, which you can jump directly to by clicking the table of contents on the right\nEach video goes through one or more Jupyter notebooks, which you’ll need to run and experiment with to get the most out of the course\nAll information needed to complete a lesson (including links to the repo with the notebooks) is in the “lesson resources” section of the lesson page\nAmongst the lesson resources you’ll find a “discuss this lesson” link, which will take you to a Q&A page on our forums for that particular lesson\nThe material covered in this course includes stuff that would normally only be included in post-graduate level programs. We try to present it in the clearest way possible, but you should expect to work hard and put in plenty of hours of study\nWe assume familiarity with the material in part 1 of this course. If you find yourself unsure about some of the foundational deep learning ideas refered to in the lessons, we’d suggest going back to study the lessons in part 1 that cover those ideas\nIf there’s mathematical or coding concepts that we use that you’re not comfortable with, don’t be afraid to seek out other tutorials to help fill in your gaps\nOn forums.fast.ai there are many other students you can collaborate with, and many folks are looking for study groups or study buddies. Studying in groups has been shown to be more effective for most people than studying alone\nIn many lessons we’ll include a challenge for you to complete, some of which involve trying novel research directions where you’ll be venturing into the academic unknown.",
    "crumbs": [
      "Part 2",
      "9: Stable Diffusion"
    ]
  },
  {
    "objectID": "Lessons/lesson9.html#what-you-need-to-know",
    "href": "Lessons/lesson9.html#what-you-need-to-know",
    "title": "9: Stable Diffusion",
    "section": "",
    "text": "Here’s what you need to know to complete this course:\n\nThe lesson is presented as a video, which you can jump directly to by clicking the table of contents on the right\nEach video goes through one or more Jupyter notebooks, which you’ll need to run and experiment with to get the most out of the course\nAll information needed to complete a lesson (including links to the repo with the notebooks) is in the “lesson resources” section of the lesson page\nAmongst the lesson resources you’ll find a “discuss this lesson” link, which will take you to a Q&A page on our forums for that particular lesson\nThe material covered in this course includes stuff that would normally only be included in post-graduate level programs. We try to present it in the clearest way possible, but you should expect to work hard and put in plenty of hours of study\nWe assume familiarity with the material in part 1 of this course. If you find yourself unsure about some of the foundational deep learning ideas refered to in the lessons, we’d suggest going back to study the lessons in part 1 that cover those ideas\nIf there’s mathematical or coding concepts that we use that you’re not comfortable with, don’t be afraid to seek out other tutorials to help fill in your gaps\nOn forums.fast.ai there are many other students you can collaborate with, and many folks are looking for study groups or study buddies. Studying in groups has been shown to be more effective for most people than studying alone\nIn many lessons we’ll include a challenge for you to complete, some of which involve trying novel research directions where you’ll be venturing into the academic unknown.",
    "crumbs": [
      "Part 2",
      "9: Stable Diffusion"
    ]
  },
  {
    "objectID": "Lessons/lesson9.html#lesson-overview",
    "href": "Lessons/lesson9.html#lesson-overview",
    "title": "9: Stable Diffusion",
    "section": "Lesson overview",
    "text": "Lesson overview\nThis lesson starts with a tutorial on how to use pipelines in the Diffusers library to generate images. Diffusers is (in our opinion!) the best library available at the moment for image generation. It has many features and is very flexible. We explain how to use its many features, and discuss options for accessing the GPU resources needed to use the library.\nWe talk about some of the nifty tweaks available when using Stable Diffusion in Diffusers, and show how to use them: guidance scale (for varying the amount the prompt is used), negative prompts (for removing concepts from an image), image initialisation (for starting with an existing image), textual inversion (for adding your own concepts to generated images), Dreambooth (an alternative approach to textual inversion).\nThe second half of the lesson covers the key concepts involved in Stable Diffusion:\n\nCLIP embeddings\nThe VAE (variational autoencoder)\nPredicting noise with the unet\nRemoving noise with schedulers\n\nJeremy shows a theoretical foundation for how Stable Diffusion works, using a novel interpretation that shows an easily-understood intuition for the theory. He introduces the concept of finite differencing and analytic derivatives, using an example of training a neural network to identify pixel adjustments to make an image look more like a handwritten digit, and describes how the derivatives of such a model can provide the score needed to provide the basis of a diffusion process that generates handwritten digits.\nThe lesson also covers finite differencing, analytic derivatives, autoencoders, and U-Nets. Jeremy introduces the concept of creating a model that can take a sentence and return a vector of numbers representing the image, using two models: a text encoder and an image encoder. The lesson concludes with a discussion of the similarities between diffusion-based models and deep learning optimizers, suggesting new research directions.",
    "crumbs": [
      "Part 2",
      "9: Stable Diffusion"
    ]
  },
  {
    "objectID": "Lessons/lesson9.html#concepts-discussed",
    "href": "Lessons/lesson9.html#concepts-discussed",
    "title": "9: Stable Diffusion",
    "section": "Concepts discussed",
    "text": "Concepts discussed\n\nStable Diffusion\nHugging Face’s Diffusers library\nPre-trained pipelines\nGuidance scale\nNegative prompts\nImage-to-image pipelines\nFinite differencing\nAnalytic derivatives\nAutoencoders\nTextual inversion\nDreambooth\nLatents\nU-Nets\nText encoders and image encoders\nContrastive loss function\nCLIP text encoder\nDeep learning optimizers\nPerceptual loss",
    "crumbs": [
      "Part 2",
      "9: Stable Diffusion"
    ]
  },
  {
    "objectID": "Lessons/lesson9.html#video",
    "href": "Lessons/lesson9.html#video",
    "title": "9: Stable Diffusion",
    "section": "Video",
    "text": "Video",
    "crumbs": [
      "Part 2",
      "9: Stable Diffusion"
    ]
  },
  {
    "objectID": "Lessons/lesson9.html#lesson-resources",
    "href": "Lessons/lesson9.html#lesson-resources",
    "title": "9: Stable Diffusion",
    "section": "Lesson resources",
    "text": "Lesson resources\n\nOther Videos\n\nLesson 9A video—Deep Dive—from @johnowhitaker (with accompanying notebook)\nLesson 9B video—The Math of Diffusion—from @seem and @ilovescience\n\nJeremy’s lesson notes\nThe fastai book:\n\nPublished version\nFree notebook version\nSample full chapters\n\nStudent notes – Lesson Notes h/t @barnacl",
    "crumbs": [
      "Part 2",
      "9: Stable Diffusion"
    ]
  },
  {
    "objectID": "Lessons/lesson9.html#links-from-the-lesson",
    "href": "Lessons/lesson9.html#links-from-the-lesson",
    "title": "9: Stable Diffusion",
    "section": "Links from the lesson",
    "text": "Links from the lesson\n\nDiscuss this lesson\nCourse repo\ndiffusion-nbs repo\nHuggingFace Notebooks\nGPU servers\n\nLambda Labs\nPaperspace Gradient\nJarvis Labs\nvast.ai - crowdsourced GPU service\n\nPrompt Engineering\n\nLexica\nPromptHero\nHexo - 10M Images and Prompts\n\nTools and Resources for AI Art\nfastai repo",
    "crumbs": [
      "Part 2",
      "9: Stable Diffusion"
    ]
  },
  {
    "objectID": "Lessons/lesson9.html#useful-background-on-fast.ai-courses",
    "href": "Lessons/lesson9.html#useful-background-on-fast.ai-courses",
    "title": "9: Stable Diffusion",
    "section": "Useful background on fast.ai courses",
    "text": "Useful background on fast.ai courses\n\nHomework\nSummaries + Things Jeremy Says to do + Qs\nFastai: A Layered API for Deep Learning paper: Information Journal or arxiv or fast.ai\nProviding a Good Education in Deep Learning: fast.ai teaching philosophy\n“How not to do fastai”\n“FastAI Lesson Zero: video + notes”",
    "crumbs": [
      "Part 2",
      "9: Stable Diffusion"
    ]
  },
  {
    "objectID": "Lessons/part2.html",
    "href": "Lessons/part2.html",
    "title": "Part 2 overview",
    "section": "",
    "text": "In this course, containing over 30 hours of video content, we implement the astounding Stable Diffusion algorithm from scratch! That’s the killer app that made the internet freak out, and caused the media to say “you may never believe what you see online again”.\nWe’ve worked closely with experts from Stability.ai and Hugging Face (creators of the Diffusers library) to ensure we have rigorous coverage of the latest techniques. The course includes coverage of papers that were released after Stable Diffusion came out – so it actually goes well beyond even what Stable Diffusion includes! We also explain how to read research papers, and practice this skill by studying and implementing many papers throughout the course.\nStable diffusion, and diffusion methods in general, are a great learning goal for many reasons. For one thing, of course, you can create amazing stuff with these algorithms! To really take the technique to the next level, and create things that no-one has seen before, you need to really deeply understand what’s happening under the hood. With this understanding, you can craft your own loss functions, initialization methods, multi-model mixups, and more, to create totally new applications that have never been seen before. Just as important: it’s a great learning goal because nearly every key technique in modern deep learning comes together in these methods. Contrastive learning, transformer models, auto-encoders, CLIP embeddings, latent variables, u-nets, resnets, and much more are involved in creating a single image.\nTo get the most out of this course, you should be a reasonably confident deep learning practitioner. If you’ve finished fast.ai’s Practical Deep Learning course then you’ll be ready! If you haven’t done that course, but are comfortable with building an SGD training loop from scratch in Python, being competitive in Kaggle competitions, using modern NLP and computer vision algorithms for practical problems, and working with PyTorch and fastai, then you will be ready to start the course. (If you’re not sure, then we strongly recommend getting starting with Practical Deep Learning.)",
    "crumbs": [
      "Part 2",
      "Part 2 overview"
    ]
  },
  {
    "objectID": "Lessons/part2.html#content-summary",
    "href": "Lessons/part2.html#content-summary",
    "title": "Part 2 overview",
    "section": "Content summary",
    "text": "Content summary\nIn this course, we’ll explore diffusion methods such as Denoising Diffusion Probabilistic Models (DDPM) and Denoising Diffusion Implicit Models (DDIM). We’ll get our hands dirty implementing unconditional and conditional diffusion models, experimenting with different samplers, and diving into recent tricks like textual inversion and Dreambooth.\nAlong the way, we’ll cover essential deep learning topics like neural network architectures, data augmentation approaches, and various loss functions. We’ll build our own models from scratch, such as Multi-Layer Perceptrons (MLPs), ResNets, and Unets, while experimenting with generative architectures like autoencoders and transformers.\nThroughout the course, we’ll use PyTorch to implement our models, and will create our own deep learning framework called miniai. We’ll master Python concepts like iterators, generators, and decorators to keep our code clean and efficient. We’ll also explore deep learning optimizers like stochastic gradient descent (SGD) accelerated approaches, learning rate annealing, and learning how to experiment with the impact different initialisers, batch sizes and learning rates. And of course, we’ll make use of handy tools like the Python debugger (pdb) and nbdev for building Python modules from Jupyter notebooks.\nLastly, we’ll touch on fundamental concepts like tensors, calculus, and pseudo-random number generation to provide a solid foundation for our exploration. We’ll apply these concepts to machine learning techniques like mean shift clustering and convolutional neural networks (CNNs), and will see how to use tracking with Weights and Biases (W&B).\nWe’ll also tackle mixed precision training using both NVIDIA’s apex library, and the Accelerate library from Hugging Face. We’ll investigate various types of normalization like Layer Normalization and Batch Normalization. By the end of the course, you’ll have a deep understanding of diffusion models and the skills to implement cutting-edge deep learning techniques.\n\nGet started now!",
    "crumbs": [
      "Part 2",
      "Part 2 overview"
    ]
  },
  {
    "objectID": "Lessons/part2.html#topics-covered",
    "href": "Lessons/part2.html#topics-covered",
    "title": "Part 2 overview",
    "section": "Topics covered",
    "text": "Topics covered\nHere’s a list of all the stuff that you’ll learn in detail and build from scratch in this course. (When we say “from scratch”, we mean that you’ll rely on nothing other than Python and its standard library.)\n\nDiffusion foundations\n\nDenoising Diffusion Probabilistic Models (DDPM)\n\nForward and reverse processes\nImplementing a noise prediction model using a neural network\nVisualizing noisy images at different timesteps\n\nDenoising Diffusion Implicit Model (DDIM)\n\nDDPM/DDIM improvements\nAlternative noise schedules\n\nPre-conditioning\nImplementation and performance of different samplers\n\nEuler sampler\nAncestral Euler sampler\nHeuns method\nLMS sampler\n\nImplementing an unconditional stable diffusion model\nCreating a conditional stable diffusion model\nInverse problems\n\nTextual inversion\nDreambooth\n\n\nHugging Face’s Diffusers library\n\nPre-trained pipelines\nImage-to-image pipelines\nGuidance scale\nNegative prompts\nCallbacks\nWorking with Hugging Face datasets\n\nDeep learning optimizers\n\nStochastic gradient descent (SGD) accelerated approaches\n\nMomentum\nRMSProp\nAdam\n\nLearning rate annealing\n\nPyTorch learning rate schedulers\n\nCosine Annealing\nOneCycleLR\n\n\nExperimenting with batch sizes and learning rates\nWorking with PyTorch optimizers\n\nPython concepts\n\nOrganizing and simplifying code\nIterators and generators in Python\nDunder methods\nPython data model\nPython debugger (pdb)\nUsing nbdev to create Python modules from Jupyter notebooks\ntry-except blocks\ndecorators\ngetattr\n**kwargs and delegates\n\nBasic foundations\n\nTensors\n\nLinear classifier using a tensor\nMatrix multiplication using Python and Numba\nComparing APL with PyTorch\nFrobenius norm\nBroadcasting in deep learning and machine learning code\nMatrix multiplication\nEinstein summation notation and torch.einsum\nGPU acceleration and CUDA\nNumba\n\nCalculus\n\nDerivatives and Infinitesimals\nFinite differencing\nAnalytic derivatives\n\nLoss functions\n\nContrastive loss function\nPerceptual loss\nlog_softmax() function and cross entropy loss\n\nPseudo-random number generation\n\nWickman-Hill algorithm\nRandom state in deep learning\n\n\nNeural network architectures\n\nMulti-Layer Perceptron (MLP) implementation\nGradients and derivatives\nChain rule and backpropagation\nPyTorch for calculating derivatives\nReLU and linear function classes\nResNets\nGenerative architectures\n\nAutoencoders\n\nConvolutional autoencoders\nVariational autoencoders\n\nUnets\n\nExperimenting with cross connections in Unets\n\nCLIP text encoders and image encoders\nTransformers\n\nSelf-attention and multi-headed attention\nRearrange function\nTime embedding and sinusoidal embeddings\n\nCreating a super-resolution U-Net model\nGradually unfreezing pre-trained networks\nStyle transfer\n\nContent loss\nGram Matrix\n\nNeural Cellular Automata\n\nCircular padding\nGradient normalization\n\n\n\nDeep learning techniques\n\nData augmentation techniques\n\nRandom erasing\nTrivialAugment\nTest time augmentation\n\nDropout for improving model performance\n\nTest time dropout for measuring model confidence\n\n\nPyTorch\n\nPyTorch’s nn.Module and nn.Sequential\n\nCreating custom PyTorch modules\nImplementing optimizers, DataLoaders, and Datasets\n\nPyTorch hooks\n\nLearner framework\n\nBuilding a flexible training framework\nCallbacks and exceptions (CancelFitException, CancelEpochException, CancelBatchException)\nMetrics and MetricsCB callback\nDeviceCB callback\nRefactoring code with context managers\nset_seed function\nCallback class and TrainLearner subclass\nHooksCallback and ActivationStats\nExperimenting with batch sizes and learning rates\ntorcheval library\n\nMachine Learning Techniques and Tools\n\nMean shift clustering\nGaussian kernel\nNorms\nLog sum exp trick\nConvolutional Neural Networks (CNNs)\n\nConvolutions and kernels\nIm2col technique\nPadding and stride in CNNs\nReceptive field\nBuilding a CNN from scratch\n\nWeights and Biases (W&B) for experiment tracking\n\nFréchet Inception Distance (FID) metric\nKernel Inception Distance (KID) metric\n\nMixed precision training\n\nAccelerate library from HuggingFace\nCollation function\n\nInitialization and normalization\n\nHistograms of activations\nGlorot (Xavier) initialization\nVariance, standard deviation, and covariance\nGeneral ReLU activation function\nLayer-wise Sequential Unit Variance (LSUV)\nLayer Normalization and Batch Normalization\nInstance Norm and Group Norm\n\n\n\n\nGet started now!\n\n(The “topics covered” list was taken from the concatenation of the topic list of each lesson, and using GPT 4 with this prompt: “The input text contains a markdown list of topics discussed in a number of deep learning and stable diffusion lessons. The topics from each lesson were concatenated together into this list, therefore it may contain duplicates (or near dupes) and is not well organised. Create an organised markdown list which groups similar topics together (using a hierarchy or markdown list items as appropriate) and combine duplicate or very similar topics.” The “content summary” section was taken from the “topics covered” list, and the GPT 4 prompt “Summarise the following markdown course outline using 3-4 paragraphs of informal prose in the style of Jeremy Howard. Do not follow the same order as the topics in the outline, but instead arrange them such that the most foundational and key topics come first.”)",
    "crumbs": [
      "Part 2",
      "Part 2 overview"
    ]
  },
  {
    "objectID": "Resources/kaggle.html",
    "href": "Resources/kaggle.html",
    "title": "Kaggle",
    "section": "",
    "text": "Kaggle is the world’s largest data science community. One of Kaggle’s features is “Notebooks”, which is “a cloud computational environment that enables reproducible and collaborative analysis”. In particular, Kaggle provides access to GPUs for free. Every lesson provides direct links to notebooks on Kaggle that are ready for you to start using. Click “Copy & Edit” at the top right of any notebook to start working with it.\nIn order to use a GPU on Kaggle, your account must be phone verified. You can enable this on your account page (after you’ve signed up and are logged in) under “Phone Verification”.\nHere’s some information from Kaggle’s notebook page:\n\nJupyter notebooks consist of a sequence of cells, where each cell is formatted in either Markdown (for writing text) or in a programming language of your choice (for writing code). To start a notebook, click on “Create Notebook”, and select “Notebook”. This will open the Notebooks editing interface.\n“Comprehensive data exploration with Python” is a great example of a Python Jupyter Notebook-type.\n\nInstead of Kaggle, you can also use Paperspace Gradient. We’ve found they work really well for this course, and have good free options. If you don’t have a Paperspace account yet, sign up with this link to get $10 credit – and we get a credit too. Gradient is a little harder to use because the notebooks are not ready-to-run, but it’s more powerful because you get a full Linux environment to work in. It’s a good option for folks who are comfortable working with git and the command line. To access the course notebooks on Paperspace, you will need to clone this GitHub repo.",
    "crumbs": [
      "Resources",
      "Kaggle"
    ]
  },
  {
    "objectID": "Resources/testimonials.html",
    "href": "Resources/testimonials.html",
    "title": "Testimonials",
    "section": "",
    "text": "“‘Deep Learning is for everyone’ we see in Chapter 1, Section 1 of this book, and while other books may make similar claims, this book delivers on the claim. The authors have extensive knowledge of the field but are able to describe it in a way that is perfectly suited for a reader with experience in programming but not in machine learning. The book shows examples first, and only covers theory in the context of concrete examples. For most people, this is the best way to learn. The book does an impressive job of covering the key applications of deep learning in computer vision, natural language processing, and tabular data processing, but also covers key topics like data ethics that some other books miss. Altogether, this is one of the best sources for a programmer to become proficient in deep learning.” – Peter Norvig, Director of Research, Google\n“As artificial intelligence has moved into the era of deep learning, it behooves all of us to learn as much as possible about how it works. Deep Learning for Coders provides a terrific way to initiate that, even for the uninitiated, achieving the feat of simplifying what most of us would consider highly complex” – Eric Topol, Author of Deep Medicine; Professor: Scripps Research\n“If you are looking for a guide that starts at the ground floor and takes you to the cutting edge of research, this is the book for you. Don’t let those PhDs have all the fun—you too can use deep learning to solve practical problems.” – Hal Varian, Emeritus Professor, UC Berkeley; Chief Economist, Google\n“Jeremy and Sylvain take you on an interactive–in the most literal sense as each line of code can be run in a notebook–journey through the loss valleys and performance peaks of deep learning. Peppered with thoughtful anecdotes and practical intuitions from years of developing and teaching machine learning, the book strikes the rare balance of communicating deeply technical concepts in a conversational and light-hearted way. In a faithful translation of fast.ai’s award-winning online teaching philosophy, the book provides you with state-of-the-art practical tools and the real-world examples to put them to use. Whether you’re a beginner or a veteran, this book will fast-track your deep learning journey and take you to new heights–and depths.” – Sebastian Ruder, Research Scientist, Deepmind\n“Jeremy Howard and Sylvain Gugger have authored a bravura of a book that successfully bridges the AI domain with the rest of the world. This work is a singularly substantive and insightful yet absolutely relatable primer on deep learning for anyone who is interested in this domain: a lodestar book amongst many in this genre.” – Anthony Chang, Chief Intelligence and Innovation Officer, Children’s Hospital of Orange County\n“How can I ‘get’ deep learning without getting bogged down? How can I quickly learn the concepts, craft, and tricks-of-the-trade using examples and code? Right here. Don’t miss the new locus classicus for hands-on deep learning” – Oren Etzioni, Professor: University of Washington, CEO: Allen Institute for AI\n“This book is a rare gem- the product of carefully crafted and highly effective teaching, iterated and refined over several years resulting in thousands of happy students. I’m one of them. fast.ai changed my life in a wonderful way, and I’m convinced that they can do the same for you.” – Jason Antic, Creator of DeOldify\n“Deep Learning for Coders is an incredible resource. The book wastes no time and teaches how to use Deep Learning effectively in the first few chapters. It then covers the inner workings of ML models and frameworks in a thorough but accessible fashion, which will allow you to understand and build upon them. I wish there was a book like this when I started learning ML, it is an instant classic!” – Emmanuel Ameisen, Author of Building Machine Learning Powered Applications\n“Gugger and Howard have created an ideal resource for anyone who has ever done even a little bit of coding. This book, and the fast.ai courses that go with it, simply and practically demystify deep learning using a hands on approach, with pre-written code that you can explore and re-use. No more slogging through theorems and proofs about abstract concepts. In Chapter 1 you will build your first deep learning model, and by the end of the book you will know how to read and understand the Methods section of any deep learning paper.” – Curtis Langlotz, Director, Center for Artificial Intelligence in Medicine and Imaging, Stanford University\n“This book demystifies the blackest of black boxes: Deep Learning. It enables quick code experimentations with a complete python notebook. It also dives into the ethical implication of Artificial Intelligence, and shows how to avoid it from becoming dystopian.” – Guillaume Chaslot , Fellow, Mozilla\n“As a pianist turned OpenAI researcher, I’m often asked for advice on getting into Deep Learning, and I always point to fastai. This book manages the seemingly impossible - it’s a friendly guide to a complicated subject, and yet it’s full of cutting-edge gems that even advanced practitioners will love.” – Christine Payne, Researcher, OpenAI; Creator of Musenet and Jukebox\n“An extremely hands-on, accessible book to help anyone quickly get started on their deep learning project. It’s a very clear, easy to follow and honest guide to practical deep learning. Helpful for beginners to executives/managers alike. The guide I wished I had years ago!” – Carol Reiley, Founding President and Chair, Drive.ai\n“Jeremy and Sylvain’s expertise in deep learning, their practical approach to ML, and their many valuable open-source contributions have made then key figures in the PyTorch community. This book, which continues the work that they and the fast.ai community are doing to make ML more accessible, will greatly benefit the entire field of AI.” – Jerome Pesenti, Vice President of AI, Facebook\n“Deep Learning is one of the most important technologies now, responsible for many amazing recent advances in AI. It used to be only for PhDs, but no longer! This book, based on a very popular fast.ai course, makes DL accessible to anyone with programming experience. This book teaches the”whole game”, with excellent hands-on examples and a companion interactive site. And PhDs will also learn a lot.” – Gregory Piatetsky-Shapiro, President, KDnuggets\n“An extension of the fast.ai course that I have consistently recommended for years, this book by Jeremy and Sylvain, two of the best Deep Learning experts today, will take you from beginner to qualified practitioner in a matter of months. Finally, something positive has come out of 2020!” – Louis Monier, Founder of Altavista; former Head of Airbnb AI Lab\n“We recommend this book! Deep Learning for Coders with fastai and PyTorch uses advanced frameworks to move quickly through concrete, real-world artificial intelligence or automation tasks. This leaves time to cover usually neglected topics, like safely taking models to production and a much-needed chapter on data ethics.” – John Mount and Nina Zumel, Authors of Practical Data Science with R\n“Deep Learning for Coders is much more than a book, as it is accompanied by fastai, a robust community and powerful machine learning framework built on pytorch. State of the art methods are provided out of the box with no compromises, including tricks to make one competitive with top industrial research labs with only a fraction of the compute. The philosophies with respect to education and learning espoused in this book and companion courses have given me the tools to accelerate my personal growth on many dimensions. Through fastai and this book, I have also learned valuable practices for software engineering, testing, iterative development, and ethical frameworks. Jeremy is an awe-inspiring individual who is not only among the top data scientists in the world but an impressive mental athlete who has mastered a wide variety of fields, and you get a glimpse into his mind in this book. Finally, Jeremy and Sylvian are exceptional in that they teach with empathy at all times, which translates into the most approachable book you can buy on deep learning today.” – Hamel Husain, Machine Learning Engineer: GitHub; Product Lead: CodeSearchNet\n“This book is”for Coders” and does not require a PhD. Now, I do have a PhD and I am no coder, so why have I been asked to review this book? Well, to tell you how friggin awesome it really is! Within a couple of pages from Chapter 1 you’ll figure out how to get a state-of-the-art network able to classify cat vs. dogs in 4 lines of code and less than 1 minute of computation. Then you land Chapter 2, which takes you from model to production, showing how you can serve a webapp in no time, without any HTML or JavaScript, without owning a server. I think of this book as an onion. A complete package that works using the best possible settings. Then, if some alterations are required, you can peel the outer layer. More tweaks? You can keep discarding shells. Even more? You can go as deep as using bare PyTorch. You’ll have three independent voices accompanying you around your journey along this 500 page book, providing you guidance and individual perspective.” – Alfredo Canziani, Professor of Computer Science, NYU\n“Deep Learning for Coders with fastai and Pytorch is an approachable conversationally-driven book that uses the whole game approach to teaching deep learning concepts. The book focuses on getting your hands dirty right out of the gate with real examples and bringing the reader along with reference concepts only as needed. A practitioner may approach the world of deep learning in this book through hands-on examples in the first half, but will find themselves naturally introduced to deeper concepts as they traverse the back half of the book with no pernicious myths left unturned.” – Josh Patterson, Patterson Consulting\n“When your model is not performing as well as you had hoped (almost always), read this book! It provides a great combination of Jeremy’s practical experience and Sylvain theoretical knowledge, and makes the art of deep learning accessible.” – Ron Kohavi, VP and Technical Fellow at Airbnb, and co-author of Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing\n“Jeremy, Sylvain and Rachel are the absolute masters of creating accessible tools and building community around AI. This is yet another installment of the fast.ai team creating an amazing resource that will help onboard the next hundred thousand aspiring AI researchers globally. Congrats!!!” – Joe Spisak, PyTorch Product Manager, Facebook\n“I am very impressed with your teaching materials. You take great care in presenting difficult topics to a varied audience. Thanks so much for sharing this.” – Brian Lovell, Professor of AI, University of Queensland\n\n\n\n\n\n\n\nGet started\n\n\n\nStart watching lesson 1 now!",
    "crumbs": [
      "Resources",
      "Testimonials"
    ]
  },
  {
    "objectID": "Resources/testimonials.html#praise-for-the-book",
    "href": "Resources/testimonials.html#praise-for-the-book",
    "title": "Testimonials",
    "section": "",
    "text": "“‘Deep Learning is for everyone’ we see in Chapter 1, Section 1 of this book, and while other books may make similar claims, this book delivers on the claim. The authors have extensive knowledge of the field but are able to describe it in a way that is perfectly suited for a reader with experience in programming but not in machine learning. The book shows examples first, and only covers theory in the context of concrete examples. For most people, this is the best way to learn. The book does an impressive job of covering the key applications of deep learning in computer vision, natural language processing, and tabular data processing, but also covers key topics like data ethics that some other books miss. Altogether, this is one of the best sources for a programmer to become proficient in deep learning.” – Peter Norvig, Director of Research, Google\n“As artificial intelligence has moved into the era of deep learning, it behooves all of us to learn as much as possible about how it works. Deep Learning for Coders provides a terrific way to initiate that, even for the uninitiated, achieving the feat of simplifying what most of us would consider highly complex” – Eric Topol, Author of Deep Medicine; Professor: Scripps Research\n“If you are looking for a guide that starts at the ground floor and takes you to the cutting edge of research, this is the book for you. Don’t let those PhDs have all the fun—you too can use deep learning to solve practical problems.” – Hal Varian, Emeritus Professor, UC Berkeley; Chief Economist, Google\n“Jeremy and Sylvain take you on an interactive–in the most literal sense as each line of code can be run in a notebook–journey through the loss valleys and performance peaks of deep learning. Peppered with thoughtful anecdotes and practical intuitions from years of developing and teaching machine learning, the book strikes the rare balance of communicating deeply technical concepts in a conversational and light-hearted way. In a faithful translation of fast.ai’s award-winning online teaching philosophy, the book provides you with state-of-the-art practical tools and the real-world examples to put them to use. Whether you’re a beginner or a veteran, this book will fast-track your deep learning journey and take you to new heights–and depths.” – Sebastian Ruder, Research Scientist, Deepmind\n“Jeremy Howard and Sylvain Gugger have authored a bravura of a book that successfully bridges the AI domain with the rest of the world. This work is a singularly substantive and insightful yet absolutely relatable primer on deep learning for anyone who is interested in this domain: a lodestar book amongst many in this genre.” – Anthony Chang, Chief Intelligence and Innovation Officer, Children’s Hospital of Orange County\n“How can I ‘get’ deep learning without getting bogged down? How can I quickly learn the concepts, craft, and tricks-of-the-trade using examples and code? Right here. Don’t miss the new locus classicus for hands-on deep learning” – Oren Etzioni, Professor: University of Washington, CEO: Allen Institute for AI\n“This book is a rare gem- the product of carefully crafted and highly effective teaching, iterated and refined over several years resulting in thousands of happy students. I’m one of them. fast.ai changed my life in a wonderful way, and I’m convinced that they can do the same for you.” – Jason Antic, Creator of DeOldify\n“Deep Learning for Coders is an incredible resource. The book wastes no time and teaches how to use Deep Learning effectively in the first few chapters. It then covers the inner workings of ML models and frameworks in a thorough but accessible fashion, which will allow you to understand and build upon them. I wish there was a book like this when I started learning ML, it is an instant classic!” – Emmanuel Ameisen, Author of Building Machine Learning Powered Applications\n“Gugger and Howard have created an ideal resource for anyone who has ever done even a little bit of coding. This book, and the fast.ai courses that go with it, simply and practically demystify deep learning using a hands on approach, with pre-written code that you can explore and re-use. No more slogging through theorems and proofs about abstract concepts. In Chapter 1 you will build your first deep learning model, and by the end of the book you will know how to read and understand the Methods section of any deep learning paper.” – Curtis Langlotz, Director, Center for Artificial Intelligence in Medicine and Imaging, Stanford University\n“This book demystifies the blackest of black boxes: Deep Learning. It enables quick code experimentations with a complete python notebook. It also dives into the ethical implication of Artificial Intelligence, and shows how to avoid it from becoming dystopian.” – Guillaume Chaslot , Fellow, Mozilla\n“As a pianist turned OpenAI researcher, I’m often asked for advice on getting into Deep Learning, and I always point to fastai. This book manages the seemingly impossible - it’s a friendly guide to a complicated subject, and yet it’s full of cutting-edge gems that even advanced practitioners will love.” – Christine Payne, Researcher, OpenAI; Creator of Musenet and Jukebox\n“An extremely hands-on, accessible book to help anyone quickly get started on their deep learning project. It’s a very clear, easy to follow and honest guide to practical deep learning. Helpful for beginners to executives/managers alike. The guide I wished I had years ago!” – Carol Reiley, Founding President and Chair, Drive.ai\n“Jeremy and Sylvain’s expertise in deep learning, their practical approach to ML, and their many valuable open-source contributions have made then key figures in the PyTorch community. This book, which continues the work that they and the fast.ai community are doing to make ML more accessible, will greatly benefit the entire field of AI.” – Jerome Pesenti, Vice President of AI, Facebook\n“Deep Learning is one of the most important technologies now, responsible for many amazing recent advances in AI. It used to be only for PhDs, but no longer! This book, based on a very popular fast.ai course, makes DL accessible to anyone with programming experience. This book teaches the”whole game”, with excellent hands-on examples and a companion interactive site. And PhDs will also learn a lot.” – Gregory Piatetsky-Shapiro, President, KDnuggets\n“An extension of the fast.ai course that I have consistently recommended for years, this book by Jeremy and Sylvain, two of the best Deep Learning experts today, will take you from beginner to qualified practitioner in a matter of months. Finally, something positive has come out of 2020!” – Louis Monier, Founder of Altavista; former Head of Airbnb AI Lab\n“We recommend this book! Deep Learning for Coders with fastai and PyTorch uses advanced frameworks to move quickly through concrete, real-world artificial intelligence or automation tasks. This leaves time to cover usually neglected topics, like safely taking models to production and a much-needed chapter on data ethics.” – John Mount and Nina Zumel, Authors of Practical Data Science with R\n“Deep Learning for Coders is much more than a book, as it is accompanied by fastai, a robust community and powerful machine learning framework built on pytorch. State of the art methods are provided out of the box with no compromises, including tricks to make one competitive with top industrial research labs with only a fraction of the compute. The philosophies with respect to education and learning espoused in this book and companion courses have given me the tools to accelerate my personal growth on many dimensions. Through fastai and this book, I have also learned valuable practices for software engineering, testing, iterative development, and ethical frameworks. Jeremy is an awe-inspiring individual who is not only among the top data scientists in the world but an impressive mental athlete who has mastered a wide variety of fields, and you get a glimpse into his mind in this book. Finally, Jeremy and Sylvian are exceptional in that they teach with empathy at all times, which translates into the most approachable book you can buy on deep learning today.” – Hamel Husain, Machine Learning Engineer: GitHub; Product Lead: CodeSearchNet\n“This book is”for Coders” and does not require a PhD. Now, I do have a PhD and I am no coder, so why have I been asked to review this book? Well, to tell you how friggin awesome it really is! Within a couple of pages from Chapter 1 you’ll figure out how to get a state-of-the-art network able to classify cat vs. dogs in 4 lines of code and less than 1 minute of computation. Then you land Chapter 2, which takes you from model to production, showing how you can serve a webapp in no time, without any HTML or JavaScript, without owning a server. I think of this book as an onion. A complete package that works using the best possible settings. Then, if some alterations are required, you can peel the outer layer. More tweaks? You can keep discarding shells. Even more? You can go as deep as using bare PyTorch. You’ll have three independent voices accompanying you around your journey along this 500 page book, providing you guidance and individual perspective.” – Alfredo Canziani, Professor of Computer Science, NYU\n“Deep Learning for Coders with fastai and Pytorch is an approachable conversationally-driven book that uses the whole game approach to teaching deep learning concepts. The book focuses on getting your hands dirty right out of the gate with real examples and bringing the reader along with reference concepts only as needed. A practitioner may approach the world of deep learning in this book through hands-on examples in the first half, but will find themselves naturally introduced to deeper concepts as they traverse the back half of the book with no pernicious myths left unturned.” – Josh Patterson, Patterson Consulting\n“When your model is not performing as well as you had hoped (almost always), read this book! It provides a great combination of Jeremy’s practical experience and Sylvain theoretical knowledge, and makes the art of deep learning accessible.” – Ron Kohavi, VP and Technical Fellow at Airbnb, and co-author of Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing\n“Jeremy, Sylvain and Rachel are the absolute masters of creating accessible tools and building community around AI. This is yet another installment of the fast.ai team creating an amazing resource that will help onboard the next hundred thousand aspiring AI researchers globally. Congrats!!!” – Joe Spisak, PyTorch Product Manager, Facebook\n“I am very impressed with your teaching materials. You take great care in presenting difficult topics to a varied audience. Thanks so much for sharing this.” – Brian Lovell, Professor of AI, University of Queensland\n\n\n\n\n\n\n\nGet started\n\n\n\nStart watching lesson 1 now!",
    "crumbs": [
      "Resources",
      "Testimonials"
    ]
  },
  {
    "objectID": "Resources/testimonials.html#in-the-news",
    "href": "Resources/testimonials.html#in-the-news",
    "title": "Testimonials",
    "section": "In the news",
    "text": "In the news\n\n\n\n\n\n\n\n\nThe Economist\n\n\n\n“This month fast.ai, an education non-profit based in San Francisco, kicked off the third year of its course in deep learning. Since its inception it has attracted more than 100,000 students, scattered around the globe from India to Nigeria. The course and others like it come with a simple proposition: there is no need to spend years obtaining a phd in order to practise deep learning. Creating software that learns can be taught as a craft, not as a high intellectual pursuit to be undertaken only in an ivory tower. Fast.ai’s course can be completed in just seven weeks.  Demystifying the subject, to make it accessible to anyone who wants to learn how to build ai software, is the aim of Jeremy Howard, who founded fast.ai with Rachel Thomas, a mathematician. He says school mathematics is sufficient. “No. Greek. Letters,” Mr Howard intones, thumping the table for punctuation.  It is working.” New schemes teach the masses to build AI\n\n\n\n\n\n\n\n\n\n\n\n\nHarvard Business Review\n\n\n\n“fast.ai… can actually get smart, motivated students to the point of being able to create industrial-grade ML deployments.” The Business of Artificial Intelligence\n\n\n\n\n\n\n\n\n\n\n\n\nMIT Tech Review\n\n\n\n“Students from Fast.ai, a small organization that runs free machine-learning courses online, just created an AI algorithm that outperforms code from Google’s researchers, according to an important benchmark.” A small team of student AI coders beats Google’s machine-learning code\n\n\n\n\n\n\n\n\n\n\nGet started\n\n\n\nStart watching lesson 1 now!",
    "crumbs": [
      "Resources",
      "Testimonials"
    ]
  },
  {
    "objectID": "Resources/testimonials.html#praise-for-the-course",
    "href": "Resources/testimonials.html#praise-for-the-course",
    "title": "Testimonials",
    "section": "Praise for the course",
    "text": "Praise for the course\n\n\n\n\n\n\n\n\nChristopher KellyCEO- Nourish, Balance, Thrive\n\n\n\nI’ve tried (and if I’m honest) failed to scale the steep deep learning curve many times. I’ve bought several books and spent entire weekends watching presentations and workshops on YouTube. My biggest fear was that my financial and time investment in this course would end up in the MOOC graveyard with all the others. Jeremy Howard (the instructor) is amongst the best teachers I’ve known. I realise with hindsight it was the equations that were preventing me from becoming a deep learning practitioner. Jeremy brought me up to speed with the state-of-the-art, and within two weeks I was in the top half of the leaderboard for three Kaggle competitions.  Many of the ideas in computer science are described using a language that makes things sound more complicated than they are. Jeremy even explained some concepts with spreadsheets, which did wonders to reassure me that I did understand what was going on inside my deep learning algorithm. Sometimes half the battle of learning a new skillset is setting up the development environment. Before starting this course, I wasted hours figuring out how to configure a productive environment. I was surprised and delighted by how easy Jeremy made setting up my deep learning environment in the cloud.  I’d recommend this MOOC to anyone looking to get started in the exciting field of deep learning. I could hardly believe my luck when I discovered that the guy I just seen give a TED Talk was teaching!\n\n\n\n\n\n\n\n\n\n\n\n\nSravya TirukkovalurVice President, Apache Sentry\n\n\n\nIf you are looking to venture into the Deep learning field, look no further and take this course. It is very hands-on and adopts a top-down approach, which means everyone irrespective of varying knowledge can get started with implementing Deep learning models immediately. Another major factor why this course is very appealing is its emphasis on social relevance. That is, how can we use this awesome technology to serve the world better?\n\n\n\n\n\n\n\n\n\n\n\n\nMatt O’BrienData Scientist, UCSF Neurology\n\n\n\nThis course filled a gap I couldn’t find anywhere else—there really is no other source where I could learn from a ‘code first’ perspective. This means you can prod, poke, and cajole these networks in different ways, and see how they respond. You can quickly feel an intuitive perspective growing as you explore.\n\n\n\n\n\n\n\n\n\n\n\n\nHelena SarinAI Artist\n\n\n\nfast.ai - it’s truly amazing how many of alumni are now well known players in AI industry, first learning AI coding hands on, through generosity and deep expertise of Rachel and Jeremy 🙏🥰\n\n\n\n\n\n\n\n\n\n\n\n\nYannet InterianAssistant Professor of Analytics, University of San Francisco\n\n\n\nI teach machine learning in a master’s degree program. After this course, I cannot ignore the new developments in deep learning—I will devote one third of my machine learning course to the subject. Also, I now have the tools to apply deep learning models to real world problems. Some of the best features of this course are the well-documented ipython notebooks containing the tricks needed to be a proficient deep learning practitioner. Overall, I was very impressed with this course.\n\n\n\n\n\n\n\n\n\n\n\n\nDennis SakvaEnergy sector analyst at Dragon Capital\n\n\n\nJeremy, your class is absolutely fantastic. I’ve been pitching it to all my ML friends. The best description of CNNs and RNNs out there. Your Excel spreadsheet on embeddings was an ‘aha’ moment for me. You’re an amazing educator. Thanks!\n\n\n\n\n\n\n\n\n\n\n\n\nSara HookerResearcher, Google Brain\n\n\n\nThis is a fantastic hands on learning experience. Like many data professionals outside of academia I found deep learning to be intimidating and opaque. This class changed that and empowered me to make deep learning part of the toolkit I use at Udemy. While there are a lot of resources available online about the theoretical underpinnings of deep learning this is the only course I have found that guides students through the implementation of fundamental deep learning frameworks.   There were three things that stood out to me that made this class special: 1) you will start coding right away and see the power of neural networks in lesson one, 2) Jeremy spends a lot of the course demystifying the subject, and in the process empowers anyone to get started in the field of deep learning, 3) many ‘tricks’ on how to optimize your architecture are passed down in rapid sequence. Save yourself a lot of time by watching this course, it will take you many more hours of trial and error to learn the same content by yourself.\n\n\n\n\n\n\n\n\n\n\n\n\nJanardhan ShettySenior Big Data Engineer at Salesforce\n\n\n\nSometimes I feared whether I would be able to solve any deep learning problems, as all the research papers I read were very mathy beyond reach of simple intuitive terms. But Jeremy and Rachel (Course Professors) believe in the theory of ‘Simple is Powerful’, by virtue of which anyone who takes this course will be able to confidently understand the simple techniques behind the ‘magic’ Deep Learning.\n\n\n\n\n\n\n\n\n\n\n\n\nDario FanucchiCo-founder and CTO at Isazi Consulting\n\n\n\nRunning a company is extremely time intensive, so I was a weary of taking on the commitment of the course. It was definitely worth it, though. It smashed my preconceptions about the technological obstructions to doing deep learning, and showed again and again examples where just a small subset of the training data and just a few epochs of training on standard GPU hardware could get most of the way towards a really good model\n\n\n\n\n\n\n\n\n\n\n\n\nTaro-Shigenori ChibaOrganizer of the SF Deep Learning Study Group\n\n\n\nIt can take years to develop the necessary skills and knowledge for Deep Learning, especially without the support of mentors and peers. Not only did Jeremy teach us the most valuable methods and practices, he provided us with an invaluable community and environment. The course exceeded my expectations and showed me first hand how both Deep Learning and ourselves could change the world for better.\n\n\n\n\n\n\n\n\n\n\n\n\nRobin Kraft (@robinkraft)Product Manager at Planet Labs (Satellites)\n\n\n\nIt was very empowering to be able to start training a model within minutes downloading the Jupyter notebooks. Jeremy and Rachel were excellent instructors and the content was high quality and enlightening. It was very cool to be able to read blogposts about the latest Deep Learning research and actually be able to understand it. I was surprised to be able to match academic results from just 2 years ago with pretty simple architectures.\n\n\n\n\n\n\n\n\n\n\n\n\nNichol BradfordExecutive Director of Transformative Tech Lab at Sofia University\n\n\n\nI’m a CEO, not a coder, so the idea that I’d be able to create a GPU deep learning server in the cloud meant learning a lot of new things—but… I did it! Jeremy Howard is an incredible instructor and is able to make what might seem like a difficult subject completely accessible. In addition, he believes in deep learning for wide audiences so has developed a practical, experience based class. I really enjoyed the classes, and used the videos to watch them all twice. The course covered cutting edge topics, and I now feel comfortable with deep learning concepts and can engage effectively in technical discussions with my data science team.\n\n\n\n\n\n\n\n\n\nGet started\n\n\n\nStart watching lesson 1 now!",
    "crumbs": [
      "Resources",
      "Testimonials"
    ]
  }
]